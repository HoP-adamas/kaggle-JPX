{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.kaggle.com/code/ikeppyo/jpx-lightgbm-demo をfork。","metadata":{}},{"cell_type":"markdown","source":"# コンテストの概要\n* 日本取引所グループ（JPX）による、2000銘柄の証券を対象にした、**証券の値段（終値）の変化率**予測。\n* **翌日から翌々日にかけての終値**の変化率が目的変数。\n* 提出データは、目的変数の値そのものではなく、目的変数の値を降順に並べた際の順位。\n* コンテストについて、より詳しく知りたい場合は[【日本語ver】Easy to understand the competition](https://www.kaggle.com/code/chumajin/ver-easy-to-understand-the-competition)が非常に役に立つと思います。","metadata":{}},{"cell_type":"markdown","source":"# ノートブックの概要\n* このノートブックでは[データ読み込み]→[データ統合]→[特徴量エンジニアリング]→[学習]→[推論・評価]→[提出]を一気通貫で行います。\n* 使用するモデルはLightGBMです。\n* モデルを3つ生成し、結果をアンサンブルして最終的な推論結果を作成します。\n\n推論・評価までの流れは以下の通りです。  \n赤背景は推論・評価推時のみ使用している関数です。青背景は提出時にも使用している関数・データとなります。  \n青背景の関数をカスタマイズすることで、様々な特徴量で精度検証ができ、また、そのままSubmitもできるようになっています。","metadata":{}},{"cell_type":"code","source":"# Image('../input/jpx-images/jpx_flow.drawio.png')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:05:39.427386Z","iopub.execute_input":"2022-06-30T09:05:39.427620Z","iopub.status.idle":"2022-06-30T09:05:39.431493Z","shell.execute_reply.started":"2022-06-30T09:05:39.427592Z","shell.execute_reply":"2022-06-30T09:05:39.430733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 入力ファイルの概要\n* `jpx-tokyo-stock-exchange-prediction/`\n    * `stock_list.csv`:  主な項目は証券コードと証券に関する属性情報。Keyは**SecuritiesCode**。\n    * `train_files/`:  学習用データ群。Dateの範囲は2017-01-04 ～ 2021-12-03。\n        * `stock_prices.csv`:  主な項目は証券コードと日単位の証券価格、および、目的変数（Target）。Keyは**Date**と**SecuritiesCode**であり、これらを連結した**RowId**も用意されている。\n        * `secondary_stock_prices.csv`:  項目は**stock_prices.csv**と同じだが、**stock_prices.csv**の対象とならなかった証券が入っている。Keyは**stock_prices.csv**と同じ。\n        * `options.csv`:  主な項目はオプション（証券用語）コードと日単位のオプション価格。Keyは**Date**と**OptionsCode**であり、これらを連結した**DateCode**も用意されている。\n        * `trades.csv`:  主な項目はマーケット毎の前営業週における取引サマリ。Keyは**Date**と**Section**。**Section**を加工することで**stock_list.csv**の**NewMarketSegment**と紐づけることができる。意味のあるレコードの発生は週次。\n        * `financials.csv`:  主な項目は四半期決算報告の内容。Keyは**Date**と**SecuritiesCode**であり、これらを連結した**DateCode**も用意されている。レコードの発生は四半期毎。\n    * `supplemental_files/`:  追加の学習用データ群。5月上旬、6月上旬、およびコンテスト終了直前に最新のデータが反映される。2022-04-05時点でのDateの範囲は2021-12-06 ～ 2022-02-28。\n        * **train_files**配下と同じ形式のファイル群が格納されている\n        * `example_test_files/`:  評価用データ群（のサンプル）\n        * **train_files**配下とほぼ（※）同じ形式のファイル群が格納されている\n        * ※ **stock_prices.csv**と**secondary_stock_prices.csv**から**Target**が削除されている点だけが**train_files**配下と異なる\n    * `data_specifications/`:  上記ファイル群の項目説明書\n        * `stock_list_spec.csv`:  **stock_list.csv**の項目説明書\n        * `stock_prices_spec.csv`:  **stock_prices.csv**、**secondary_stock_prices.csv**の項目説明書\n        * `options_spec.csv`:  **options.csv**の項目説明書\n        * `trades_spec.csv`:  **trades.csv**の項目説明書\n        * `stock_fin_spec.csv`:  **financials.csv**の項目説明書\n    * `jpx_tokyo_market_prediction/`:時系列API\n    \nファイル間の関係性は以下のようなイメージです","metadata":{}},{"cell_type":"code","source":"# Image('../input/jpx-images/jpx_files.drawio.png')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:05:39.432800Z","iopub.execute_input":"2022-06-30T09:05:39.433644Z","iopub.status.idle":"2022-06-30T09:05:39.441446Z","shell.execute_reply.started":"2022-06-30T09:05:39.433603Z","shell.execute_reply":"2022-06-30T09:05:39.440927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"データの詳細は https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction/data を参照  \n時系列APIの使い方は https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction/overview/evaluation を参照  \nオプションコード体系の日本語版は https://www.jpx.co.jp/sicc/securities-code/nlsgeu0000032d48-att/(HP)sakimono20220208.pdf を参照  \n証券コード関係は https://www.jpx.co.jp/sicc/securities-code/01.html を参照","metadata":{}},{"cell_type":"markdown","source":"# 準備\n* 使用するライブラリをインポートします","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nfrom pathlib import Path\nfrom decimal import ROUND_HALF_UP, Decimal\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:05:39.442921Z","iopub.execute_input":"2022-06-30T09:05:39.443380Z","iopub.status.idle":"2022-06-30T09:05:40.759811Z","shell.execute_reply.started":"2022-06-30T09:05:39.443341Z","shell.execute_reply":"2022-06-30T09:05:40.758737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if (c_min > np.iinfo(np.int8).min) & (c_max < np.iinfo(np.int8).max):\n                    df[col] = df[col].astype(np.int8)\n                elif (c_min > np.iinfo(np.int16).min) & (c_max < np.iinfo(np.int16).max):\n                    df[col] = df[col].astype(np.int16)\n                elif (c_min > np.iinfo(np.int32).min) & (c_max < np.iinfo(np.int32).max):\n                    df[col] = df[col].astype(np.int32)\n                elif (c_min > np.iinfo(np.int64).min) & (c_max < np.iinfo(np.int64).max):\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if (c_min > np.finfo(np.float16).min) & (c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > np.finfo(np.float32).min) & (c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:05:40.762175Z","iopub.execute_input":"2022-06-30T09:05:40.762555Z","iopub.status.idle":"2022-06-30T09:05:40.779580Z","shell.execute_reply.started":"2022-06-30T09:05:40.762504Z","shell.execute_reply":"2022-06-30T09:05:40.778922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# データ読み込み\n\n以下のファイル群を読み込みます。\n* stock_list.csv\n* train_files 配下のファイル（Dateの範囲は2017-01-04 ～ 2021-12-03）\n* supplemental_files 配下のファイル（Dateの範囲は2021-12-06 ～ 2022-02-28 ※2022/04/05現在）","metadata":{}},{"cell_type":"markdown","source":"## Function","metadata":{}},{"cell_type":"code","source":"def read_files(dir_name: str = 'train_files'):\n    base_path = Path(f'../input/jpx-tokyo-stock-exchange-prediction/{dir_name}')\n    prices = pd.read_csv(base_path / 'stock_prices.csv')\n    options = pd.read_csv(base_path / 'options.csv')\n    financials = pd.read_csv(base_path / 'financials.csv')\n    trades = pd.read_csv(base_path / 'trades.csv')\n    secondary_prices = pd.read_csv(base_path / 'secondary_stock_prices.csv')\n    \n    prices = reduce_mem_usage(prices)\n    options = reduce_mem_usage(options)\n    financials = reduce_mem_usage(financials)\n    trades = reduce_mem_usage(trades)\n    secondary_prices = reduce_mem_usage(secondary_prices)\n    \n    return prices, options, financials, trades, secondary_prices","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:05:40.780729Z","iopub.execute_input":"2022-06-30T09:05:40.781296Z","iopub.status.idle":"2022-06-30T09:05:40.794314Z","shell.execute_reply.started":"2022-06-30T09:05:40.781258Z","shell.execute_reply":"2022-06-30T09:05:40.793625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exec","metadata":{}},{"cell_type":"code","source":"%%time\n\nstock_list = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv')\ntrain_files = read_files('train_files')\nsupplemental_files = read_files('supplemental_files')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:05:40.796149Z","iopub.execute_input":"2022-06-30T09:05:40.796726Z","iopub.status.idle":"2022-06-30T09:06:36.355604Z","shell.execute_reply.started":"2022-06-30T09:05:40.796693Z","shell.execute_reply":"2022-06-30T09:06:36.354714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# データ統合\n\n* 読み込んだデータを結合して、一つのファイルに纏めます。","metadata":{}},{"cell_type":"markdown","source":"## Function","metadata":{}},{"cell_type":"markdown","source":"`merge_data`関数では各ファイルを水平方向に結合します。現時点では`stock_prices`、`stock_list`しか使っていません。  \nコメントアウトを解除すれば`trades`、`financials`とも結合は可能ですが、  \nこれらのデータは、有効なレコードが発生するタイミングが日次ではないため、学習データとして意味のあるものとするためには「直近に発生した有効なレコードの値を引き継ぐ」などの対処が必要となります。  \n`options`に関しては、[OptionsCodeの附番規則](https://www.jpx.co.jp/sicc/securities-code/nlsgeu0000032d48-att/(HP)sakimono20220208.pdf)を見ていけば適切な使い方が見えそうです。","metadata":{}},{"cell_type":"code","source":"def merge_data(prices, options, financials, trades, secondary_prices, stock_list):\n    # stock_prices がベース\n    base_df = prices.copy()\n    \n    # stock_listと結合\n    _stock_list = stock_list.copy()\n    _stock_list.rename(columns={'Close': 'Close_x'}, inplace=True)\n    base_df = base_df.merge(_stock_list, on='SecuritiesCode', how=\"left\")\n\n    # tradesと結合\n    # stock_listのNewMarketSegmentと紐づくよう、tradesのSection項目を編集する\n    # _trades = trades.copy()\n    # _trades['NewMarketSegment'] = _trades['Section'].str.split(' \\(', expand=True)[0]\n    # base_df = base_df.merge(_trades, on=['Date', 'NewMarketSegment'], how=\"left\")\n\n    # financialsと結合\n    _financials = financials.copy()\n    _financials.rename(columns={'Date': 'Date_x', 'SecuritiesCode': 'SecuritiesCode_x'}, inplace=True)\n    use_cols = ['DateCode',\n               'NetSales',\n               'OperatingProfit',\n               'OrdinaryProfit',\n               'Profit', \n               'EarningsPerShare', \n               'TotalAssets', \n               'Equity', \n               'EquityToAssetRatio', \n               'NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock',\n               'ForecastEarningsPerShare']\n    # メモリがカツカツなのでカラムを使うものだけに絞る\n    _financials = _financials[use_cols]\n    _financials.replace(['-', '－'], np.nan, inplace=True)\n    for col in use_cols[1:]:\n        _financials[col] = _financials[col].astype(float)\n    prev_column = set(_financials.columns)\n    \n    # 同じDateCodeで複数行あるのでまとめあげる\n    _financials = _financials.groupby('DateCode').max().reset_index()\n    assert prev_column == set(_financials.columns.to_list())\n    _financials.drop_duplicates(subset='DateCode', inplace=True)\n    assert prev_column == set(_financials.columns.to_list())\n    base_df = base_df.merge(_financials, left_on='RowId', right_on='DateCode', how=\"left\")\n    \n    return base_df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:06:36.356770Z","iopub.execute_input":"2022-06-30T09:06:36.356986Z","iopub.status.idle":"2022-06-30T09:06:36.369425Z","shell.execute_reply.started":"2022-06-30T09:06:36.356959Z","shell.execute_reply":"2022-06-30T09:06:36.368536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`adjust_price`関数は[Train Demo](https://www.kaggle.com/code/smeitoma/train-demo)で紹介されている関数をほぼそのまま使わせて頂いています。（Dateのインデックス化だけコメントアウトしています）  \n項目追加が発生するため、統合の範囲を超えていますが、関数内でソートやインデックスの生成といった操作が行われるため、この段階で実行しています。  \n  \n関数では**AdjustedClose**という項目が生成されます。  \n株式は、分割や併合によって株価が大きく変動することがありますが、**Close**の代わりに**AdjustedClose**を使うことで、この影響を減少させることができるとのことです。","metadata":{}},{"cell_type":"code","source":"def adjust_price(price):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n    # transform Date column into datetime\n    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n\n    def generate_adjusted_close(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n\n    # price.set_index(\"Date\", inplace=True)\n    return price","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:06:36.370904Z","iopub.execute_input":"2022-06-30T09:06:36.371170Z","iopub.status.idle":"2022-06-30T09:06:36.390840Z","shell.execute_reply.started":"2022-06-30T09:06:36.371139Z","shell.execute_reply":"2022-06-30T09:06:36.389584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collector(prices, options, financials, trades, secondary_prices, stock_list):\n    # 読み込んだデータを統合して一つのファイルに纏める\n    base_df = merge_data(prices, options, financials, trades, secondary_prices, stock_list)\n    \n    # AdjustedClose項目の生成\n    base_df = adjust_price(base_df)\n    \n    return base_df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:06:36.392066Z","iopub.execute_input":"2022-06-30T09:06:36.392736Z","iopub.status.idle":"2022-06-30T09:06:36.406770Z","shell.execute_reply.started":"2022-06-30T09:06:36.392698Z","shell.execute_reply":"2022-06-30T09:06:36.405922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exec","metadata":{}},{"cell_type":"markdown","source":"supplemental filesを使わない場合は4行目以降をコメントアウトします。","metadata":{}},{"cell_type":"code","source":"%%time\n\nbase_df = collector(*train_files, stock_list)\nsupplemental_df = collector(*supplemental_files, stock_list)\nbase_df = pd.concat([base_df, supplemental_df]).reset_index(drop=True)\n\nbase_df['Target'] = base_df['Target'] * 100","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:06:36.407929Z","iopub.execute_input":"2022-06-30T09:06:36.408509Z","iopub.status.idle":"2022-06-30T09:07:32.590999Z","shell.execute_reply.started":"2022-06-30T09:06:36.408468Z","shell.execute_reply":"2022-06-30T09:07:32.590161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:07:32.592377Z","iopub.execute_input":"2022-06-30T09:07:32.592769Z","iopub.status.idle":"2022-06-30T09:07:32.781482Z","shell.execute_reply.started":"2022-06-30T09:07:32.592735Z","shell.execute_reply":"2022-06-30T09:07:32.780714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 特徴量エンジニアリング\n\n特徴量を生成し、推論結果の精度向上に貢献するものだけを選びます。  ","metadata":{}},{"cell_type":"markdown","source":"## Function\n\n`calc_change_rate_base`関数と`calc_volatility_base`関数は、[Train Demo](https://www.kaggle.com/code/smeitoma/train-demo)で紹介されていた関数を参考にしています。","metadata":{}},{"cell_type":"code","source":"def add_date_features(df):\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n#     df[\"Year\"] = df[\"Date\"].dt.year.astype(\"int32\")\n    df['Month'] = df[\"Date\"].dt.month.astype(\"int8\")\n    df['day_of_month'] = df[\"Date\"].dt.day.astype(\"int8\")\n    df['day_of_year'] = df[\"Date\"].dt.dayofyear.astype(\"int16\")\n    df['day_of_week'] = (df[\"Date\"].dt.dayofweek + 1).astype(\"int8\")\n    return df\n\ndef calc_return(tmp):\n    # 今回の処理では関数を入れ子にするときに引数がないと、実行するときに引数ないのに渡されてると怒られるので、使っていないけど引数を置いておく。\n    def func(price):\n        price.loc[:, 'return'] = price['AdjustedClose'].pct_change()\n        return price\n    return func\n# 「column_name で指定された項目の、periodsで指定された期間（複数）での変化率を導出し、項目として追加する関数」を生成する関数\n# 生成された関数は、特定証券コードだけを持つデータフレームが入力されることを前提としている。\ndef calc_change_rate_base(column_name, periods):\n    def func(price):\n        for period in periods:\n            price.loc[:, f\"{column_name}_change_rate_{period}\"] = price[column_name].pct_change(period)\n        return price\n    return func\n\n\n# 「column_name で指定された項目の、periodsで指定された期間（複数）での変動の度合いを導出し、項目として追加する関数」を生成する関数\n# 生成された関数は、特定証券コードだけを持つデータフレームが入力されることを前提としている。\ndef calc_volatility_base(column_name, periods):\n    def func(price):\n        for period in periods:\n            price.loc[:, f\"{column_name}_volatility_{period}\"] = np.log(price[column_name]).diff().rolling(window=period, min_periods=1).std()\n        return price\n    return func\n\n# 「column_name で指定された項目の、periodsで指定された期間（複数）での移動平均値と現在値の比率を導出し、項目として追加する関数」を生成する関数\n# 移動平均値そのものではなく、現在値に対する比率としているのは、今回のTargetが比率であるため。\n# 生成された関数は、特定証券コードだけを持つデータフレームが入力されることを前提としている。\ndef calc_moving_average_rate_base(column_name, periods):\n    def func(price):\n        for period in periods:\n            price.loc[:, f\"{column_name}_average_rate_{period}\"] = price[column_name].rolling(window=period, min_periods=1).mean()\n#             price.loc[:, f\"{column_name}_average_deviation_rate_{period}\"] = (price[column_name] - price[column_name].rolling(window=period, min_periods=1).mean()) / (price[column_name].rolling(window=period, min_periods=1).mean() + 0.00001)\n#         short_period = periods[0]\n#         mid_period = periods[1]\n#         s = price[f\"{column_name}_average_rate_{short_period}\"] - price[f\"{column_name}_average_rate_{mid_period}\"]\n#         s = s / s.abs()\n#         price.loc[:, 'moving_average_cross'] = s - s.shift(1)\n\n        return price\n    return func\n\ndef calc_rolling_features_base(column_name, periods):\n    def func(price):\n        for period in periods:\n            price.loc[:, f\"{column_name}_rolling_min_{period}\"] = price[column_name].rolling(window=period, min_periods=1).min()\n            price.loc[:, f\"{column_name}_rolling_max_{period}\"] = price[column_name].rolling(window=period, min_periods=1).max()\n            price.loc[:, f\"{column_name}_rolling_std_{period}\"] = price[column_name].rolling(window=period, min_periods=1).std().astype(np.float32)\n            price.loc[:, f\"{column_name}_rolling_var_{period}\"] = price[column_name].rolling(window=period, min_periods=1).var().astype(np.float32)\n            price.loc[:, f\"{column_name}_rolling_SR_{period}\"] = price.loc[:, f\"{column_name}_average_rate_{period}\"] / price.loc[:, f\"{column_name}_rolling_std_{period}\"]\n        price.loc[:, f'{column_name}_std_change_rate'] = price.loc[:, f\"{column_name}_rolling_std_{period}\"].pct_change()\n        return price\n    return func\n\n\n# # 終値の変動率を生成し、項目として追加する関数。これをShift-2するとTargetになる。\n# # この関数は、特定証券コードだけを持つデータフレームが入力されることを前提としている。\n# def calc_target_shift2(price):\n#     price.loc[:, 'Target_shift2'] = price['Close'].pct_change()\n#     return price\n\ndef calc_target_shifts(price):\n    lags = [1, 2, 3, 4, 5, 6, 7]\n    for i in lags:\n        price.loc[:, f'Target_shift{i}'] = price['Target'].shift(i)\n    return price\n\ndef calc_33Sector_rolling_features(base_df, periods):\n    # 33SectorCodeの平均リターンを使った特徴量\n    base_df2 = base_df.copy()\n    sector_df = base_df2.groupby(['33SectorCode', 'Date'])['return'].mean().reset_index()\n    for period in periods:\n        sector_df[f'33Sector_rolling_mean_{period}'] = sector_df.groupby(['33SectorCode'])['return'].rolling(window=period).mean().reset_index()['return']\n        \n    base_df = base_df.merge(sector_df[['33SectorCode', 'Date', '33Sector_rolling_mean_5', '33Sector_rolling_mean_20', '33Sector_rolling_mean_80']],\n                            on=['33SectorCode', 'Date'],\n                           how='left')\n    # returnが0のときに発散するのを防ぐために下駄を履かせる\n\n    for period in periods:\n        constant = abs(base_df[f'33Sector_rolling_mean_{period}'].min()+0.0001)\n        base_df[f'deviation_from_sector_MA_{period}'] = (base_df['return'] - base_df[f'33Sector_rolling_mean_{period}']) / (base_df[f'33Sector_rolling_mean_{period}'] + constant)\n    \n    return base_df\n\n#financialsデータを使った特徴量\ndef add_fin_features(price):\n#     def func(price):\n    # ここで行うのは微妙だが良さそうな場所もないので、ここでfinancialsのffillをやる。\n    f_columns = ['NetSales',\n               'OperatingProfit',\n               'OrdinaryProfit',\n               'Profit', \n               'EarningsPerShare', \n               'TotalAssets', \n               'Equity', \n               'EquityToAssetRatio', \n               'NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock',\n                'ForecastEarningsPerShare']\n    for column in f_columns:\n        price[column].fillna(method='ffill', inplace=True)\n    #自己資本(CapitalAdequacy)の計算\n    price.loc[:, 'TotalAssets']=pd.to_numeric(price.loc[:, 'TotalAssets'], errors=\"coerce\")\n    price.loc[:, 'EquityToAssetRatio']=pd.to_numeric(price.loc[:, 'EquityToAssetRatio'], errors=\"coerce\")\n    price.loc[:, \"CapitalAdequacy\"]=price.loc[:,'EquityToAssetRatio']*price.loc[:,'TotalAssets']\n#     base_df[\"CapitalAdequacy\"].fillna(method='ffill', inplace=True)\n    #時価総額(AMV)の計算\n    price.loc[:, \"NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock\"]=pd.to_numeric(price.loc[:, 'NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock'], errors=\"coerce\")\n    price.loc[:, \"AMV\"]=price.loc[:,'NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock']*price.loc[:,\"Close\"]\n#     base_df[\"AMV\"].fillna(method='ffill', inplace=True)\n    #時価簿価比率:簿価(総資産)と時価の差分を表す．この差分は、投資家の企業の将来に対する期待と解釈される\n    price.loc[:, \"PBR\"]=price.loc[:,\"AMV\"]/price.loc[:,'CapitalAdequacy']\n#     base_df[\"PBR\"].fillna(method='ffill', inplace=True)\n    #負債時価総額比率:資金調達方法(負債と株式発行)の割合．値が大きいほど負債が多く、倒産リスクが大きくなる\n    price.loc[:, 'Equity']=pd.to_numeric(price.loc[:, 'Equity'], errors=\"coerce\")\n    price.loc[:, \"debt\"]=price.loc[:,\"TotalAssets\"]-price.loc[:,\"Equity\"]\n    price.loc[:, \"RatioOfNetWorthToTotalDebt\"]=price.loc[:, \"debt\"]/price.loc[:, 'AMV']\n#     base_df[\"RatioOfNetWorthToTotalDebt\"].fillna(method='ffill', inplace=True)\n    price.drop(\"debt\", axis=1, inplace=True)\n    #営業マージン:どの程度効率よく付加価値をつけているか\n    price.loc[:, \"OperatingProfit\"]=pd.to_numeric(price.loc[:, \"OperatingProfit\"], errors=\"coerce\")\n    price.loc[:, \"NetSales\"]=pd.to_numeric(price.loc[:, \"NetSales\"], errors=\"coerce\")\n    price.loc[:, \"OperatingMargin\"]=price.loc[:,\"OperatingProfit\"]/price.loc[:,\"NetSales\"]\n#     base_df[\"OperatingMargin\"].fillna(method='ffill', inplace=True)\n    #純利益マージン；営業マージンとほとんど同じ．違いは負債などに対する支払いの有無．つまり、資金調達の効率性も加味される\n    price.loc[:, \"Profit\"]=pd.to_numeric(price.loc[:, \"Profit\"], errors=\"coerce\")\n    price.loc[:, \"ProfitMargin\"]=price.loc[:,\"Profit\"]/price.loc[:,\"NetSales\"]\n#     base_df[\"ProfitMargin\"].fillna(method='ffill', inplace=True)\n    #自己資本利益率(ROE)\n    price.loc[:, \"ROE\"]=price.loc[:,\"Profit\"]/price.loc[:,\"Equity\"]\n#     base_df[\"ROE\"].fillna(method='ffill', inplace=True)\n    #総資産回転率:保有資産の効率性を表す\n    price.loc[:, \"TotalAssetsTurnover\"]=price.loc[:,\"NetSales\"]/price.loc[:,\"TotalAssets\"]\n#     base_df[\"TotalAssetsTurnover\"].fillna(method='ffill', inplace=True)\n    #自己資本倍率\n    price.loc[:, \"CapitalAdequecyRatio\"]=price.loc[:,\"Profit\"]/price.loc[:,\"TotalAssets\"]\n#     base_df[\"CapitalAdequecyRatio\"].fillna(method='ffill', inplace=True)\n    #無リスク利子率\n    r = {2017:-0.09, 2018:-0.137, 2019:-0.18, 2020:-0.147, 2021:-0.123, 2022:-0.072}\n    price['r'] = price[\"Date\"].dt.year.astype(\"int32\")\n    price['r'] = price['r'].map(r)\n\n    #二年間の企業価値\n    price['EarningsPerShare']=pd.to_numeric(price['EarningsPerShare'], errors=\"coerce\")\n    price['ForecastEarningsPerShare']=pd.to_numeric(price['ForecastEarningsPerShare'], errors=\"coerce\")\n    price[\"NPV\"]=price.loc[:,'EarningsPerShare']+price.loc[:,'ForecastEarningsPerShare']/(1+price.loc[:,\"r\"])\n    \n#     #市場リスクプレミアム\n    market_df = price.groupby('Date')['return'].mean().reset_index()\n    market_df = market_df.rename(columns={'return':'MarketAverageReturn'})\n    price = price.merge(market_df, on=['Date'], how='left')\n    price.loc[:, 'RiskPremium'] = price['MarketAverageReturn'] - price['r']\n    price.drop('r', axis = 1, inplace=True)\n    return price\n#     return func\n\n# 入力データフレームを証券コード毎にグルーピングし、引数で渡された関数を適用する関数\n# functionsには↑で定義したcalc_xxxの関数のリストが渡される想定。\ndef add_columns_per_code(price, functions):\n    def func(df):\n        for f in functions:\n            df = f(df)\n        return df\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(func)\n    price = price.reset_index(drop=True)\n    return price\n\n# 入力データフレームに特徴量を追加する関数\n# 追加する項目は、基本的にレコード内の値だけを使う想定\ndef add_columns_per_day(base_df):\n    base_df['diff_rate1'] = (base_df['Close'] - base_df['Open']) / base_df['Close']\n    base_df['diff_rate2'] = (base_df['High'] - base_df['Low']) / base_df['Close']    \n    return base_df\n\n# 入力データフレームに特徴量を追加する関数\ndef generate_features(base_df):\n    prev_column_names = base_df.columns\n    \n    periods = [5, 20, 80]\n    functions = [\n        calc_return(periods),\n#         calc_change_rate_base(\"AdjustedClose\", periods), \n#         calc_volatility_base(\"AdjustedClose\", periods), \n#         calc_moving_average_rate_base(\"Volume\", periods), \n        calc_moving_average_rate_base(\"return\", periods), \n        calc_rolling_features_base(\"return\", periods)\n#         calc_target_shifts\n    ]\n    # 証券コード単位の特徴量（移動平均等、一定期間のレコードをインプットに生成する特徴量）を追加\n    base_df = add_columns_per_code(base_df, functions)\n    \n    # 日付の特徴量\n    base_df = add_date_features(base_df)\n    \n    # 33SectorCodeごとのリターンの移動平均\n    base_df = calc_33Sector_rolling_features(base_df, periods)\n    \n    # 財務諸表からの特徴量\n    base_df = base_df.sort_values([\"SecuritiesCode\", \"Date\"])\n    base_df = base_df.groupby(\"SecuritiesCode\").apply(add_fin_features)\n    base_df = base_df.reset_index(drop=True)\n    \n\n    # 日単位の特徴量（レコード内の値で導出できる特徴量）を追加\n    base_df = add_columns_per_day(base_df)\n    \n    # 後で特徴量を選択しやすくするため、追加した項目名のリストを生成\n    add_column_names = list(set(base_df.columns) - set(prev_column_names))\n    return base_df, add_column_names","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:07:32.785516Z","iopub.execute_input":"2022-06-30T09:07:32.785874Z","iopub.status.idle":"2022-06-30T09:07:32.840184Z","shell.execute_reply.started":"2022-06-30T09:07:32.785836Z","shell.execute_reply":"2022-06-30T09:07:32.839299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 特徴量選択\ndef select_features(feature_df, add_column_names, is_train):\n    # 基本項目\n    base_cols = ['RowId', 'Date', 'SecuritiesCode']\n    # 数値系の特徴量\n    numerical_cols = sorted(add_column_names) + ['AdjustedClose']\n    # カテゴリ系の特徴量\n    categorical_cols = ['33SectorCode']\n    # 目的変数\n    label_col = ['Target']\n    \n    # 特徴量\n    feat_cols = numerical_cols + categorical_cols\n\n    # データフレームの項目を選択された項目だけに絞込\n    feature_df = feature_df[base_cols + feat_cols + label_col]\n    # カテゴリ系項目はdtypeをcategoryに変更\n    feature_df[categorical_cols] = feature_df[categorical_cols].astype('category')\n    \n    feature_df = feature_df.drop_duplicates(subset='RowId')\n    \n    if is_train:\n        # 学習データの場合は、NA項目があるレコードを削除\n#         feature_df[numerical_cols] = feature_df[numerical_cols].fillna(method='ffill')\n        feature_df.dropna(inplace=True)\n    else:\n        # 推論データの場合は、NA項目を補完\n        feature_df[numerical_cols] = feature_df[numerical_cols].fillna(0)\n        feature_df[numerical_cols] = feature_df[numerical_cols].replace([np.inf, -np.inf], 0)\n    \n    return feature_df, feat_cols, label_col","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:07:32.844119Z","iopub.execute_input":"2022-06-30T09:07:32.844768Z","iopub.status.idle":"2022-06-30T09:07:32.859358Z","shell.execute_reply.started":"2022-06-30T09:07:32.844727Z","shell.execute_reply":"2022-06-30T09:07:32.858595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessor(base_df, is_train=True):\n    feature_df = base_df.copy()\n    \n    ## 特徴量生成\n    feature_df, add_column_names = generate_features(feature_df)\n    \n    ## 特徴量選択\n    feature_df, feat_cols, label_col = select_features(feature_df, add_column_names, is_train)\n    \n    feature_df = reduce_mem_usage(feature_df)\n\n    return feature_df, feat_cols, label_col","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:07:32.861354Z","iopub.execute_input":"2022-06-30T09:07:32.862010Z","iopub.status.idle":"2022-06-30T09:07:32.877154Z","shell.execute_reply.started":"2022-06-30T09:07:32.861962Z","shell.execute_reply":"2022-06-30T09:07:32.876229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exec","metadata":{}},{"cell_type":"code","source":"%%time\n\nfeature_df, feat_cols, label_col = preprocessor(base_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:07:32.880135Z","iopub.execute_input":"2022-06-30T09:07:32.880425Z","iopub.status.idle":"2022-06-30T09:10:12.658919Z","shell.execute_reply.started":"2022-06-30T09:07:32.880384Z","shell.execute_reply":"2022-06-30T09:10:12.657593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:10:12.660838Z","iopub.execute_input":"2022-06-30T09:10:12.661421Z","iopub.status.idle":"2022-06-30T09:10:12.806048Z","shell.execute_reply.started":"2022-06-30T09:10:12.661373Z","shell.execute_reply":"2022-06-30T09:10:12.804819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:10:12.807571Z","iopub.execute_input":"2022-06-30T09:10:12.808043Z","iopub.status.idle":"2022-06-30T09:10:12.898505Z","shell.execute_reply.started":"2022-06-30T09:10:12.807990Z","shell.execute_reply":"2022-06-30T09:10:12.897882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_cols","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:10:12.899703Z","iopub.execute_input":"2022-06-30T09:10:12.900695Z","iopub.status.idle":"2022-06-30T09:10:12.908013Z","shell.execute_reply.started":"2022-06-30T09:10:12.900656Z","shell.execute_reply":"2022-06-30T09:10:12.906784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_rank_with_new_col_name(df, col_name, new_col_name, ascend=False):\n    df[new_col_name] = df.groupby(\"Date\")[col_name].rank(ascending=ascend, method=\"first\") - 1 \n    df[new_col_name] = df[new_col_name].astype(\"int\")\n    return df\n\ndef calc_spread_return_sharpe(df: pd.DataFrame, pred_name, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df[pred_name].min() == 0\n        assert df[pred_name].max() == len(df[pred_name]) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by=pred_name)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by=pred_name, ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:10:12.909638Z","iopub.execute_input":"2022-06-30T09:10:12.909897Z","iopub.status.idle":"2022-06-30T09:10:12.924992Z","shell.execute_reply.started":"2022-06-30T09:10:12.909866Z","shell.execute_reply":"2022-06-30T09:10:12.923982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 学習","metadata":{}},{"cell_type":"markdown","source":"## Function","metadata":{}},{"cell_type":"markdown","source":"学習を行いモデルを生成します","metadata":{}},{"cell_type":"code","source":"# 予測値を降順に並べて順位番号を振る関数\n# 言い換えると、目的変数から提出用項目を導出する関数\ndef add_rank(df, col_name=\"pred\", ascend=False):\n    df[\"Rank\"] = df.groupby(\"Date\")[col_name].rank(ascending=ascend, method=\"first\") - 1 \n    df[\"Rank\"] = df[\"Rank\"].astype(\"int\")\n    return df\n\ndef add_bottom_rank(df, col_name=\"pred\"):\n    df['BottomRank'] = df.groupby(\"Date\")[col_name].rank(ascending=False, method=\"first\") - 1\n    df['BottomRank'] = df['BottomRank'].astype('int')\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:10:12.927507Z","iopub.execute_input":"2022-06-30T09:10:12.928331Z","iopub.status.idle":"2022-06-30T09:10:12.945362Z","shell.execute_reply.started":"2022-06-30T09:10:12.928278Z","shell.execute_reply":"2022-06-30T09:10:12.944630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`calc_spread_return_sharpe`関数は[Train Demo](https://www.kaggle.com/code/smeitoma/train-demo)で紹介されている関数をそのまま使わせて頂いています。  \n推論した**Rank**と、正解の**Target**を含むデータフレームを渡すと、コンテストの評価方法に沿ったスコアを計算してくれます。","metadata":{}},{"cell_type":"code","source":"def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:10:12.946947Z","iopub.execute_input":"2022-06-30T09:10:12.947419Z","iopub.status.idle":"2022-06-30T09:10:12.968712Z","shell.execute_reply.started":"2022-06-30T09:10:12.947370Z","shell.execute_reply":"2022-06-30T09:10:12.967633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 予測用のデータフレームと、予測結果をもとに、スコアを計算する関数\ndef evaluator(df, pred):\n    df[\"pred\"] = pred\n    df = add_rank(df)\n    score = calc_spread_return_sharpe(df)\n    return score","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:10:12.970046Z","iopub.execute_input":"2022-06-30T09:10:12.971472Z","iopub.status.idle":"2022-06-30T09:10:12.985769Z","shell.execute_reply.started":"2022-06-30T09:10:12.971284Z","shell.execute_reply":"2022-06-30T09:10:12.984590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`lightgbm`ではなく`optuna.integration.lightgbm`をimportすることで、パイパーパラメータチューニングが実行されるようになります。","metadata":{}},{"cell_type":"code","source":"def add_rank_with_new_col_name(df, col_name, new_col_name, ascend=False):\n    df[new_col_name] = df.groupby(\"Date\")[col_name].rank(ascending=ascend, method=\"first\") - 1 \n    df[new_col_name] = df[new_col_name].astype(\"int\")\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:10:12.988669Z","iopub.execute_input":"2022-06-30T09:10:12.988959Z","iopub.status.idle":"2022-06-30T09:10:13.006065Z","shell.execute_reply.started":"2022-06-30T09:10:12.988927Z","shell.execute_reply":"2022-06-30T09:10:13.004748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_preliminary_rank(df):\n    \n    def func(row):\n        THRESHOLD = 400\n        MAX_COUNT = 3000  # 多めにしておく\n        if row['TopPredRank'] < THRESHOLD and row['BottomPredRank'] >= THRESHOLD:\n            return row['TopPredRank']\n        elif row['TopPredRank'] < THRESHOLD and row['BottomPredRank'] >= THRESHOLD:\n            return MAX_COUNT - row['BottomPredRank']\n        elif row['TopPredRank'] >= THRESHOLD and row['BottomPredRank'] >= THRESHOLD:\n            return row['TopPredRank']\n        else:\n            if row['TopPredRank'] <= row['BottomPredRank']:\n                return row['TopPredRank']\n            else:\n                return MAX_COUNT - row['BottomPredRank']\n            \n    df.loc[:, 'PreliminaryRank'] = df.apply(lambda x: func(x), axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:10:13.007483Z","iopub.execute_input":"2022-06-30T09:10:13.007780Z","iopub.status.idle":"2022-06-30T09:10:13.021591Z","shell.execute_reply.started":"2022-06-30T09:10:13.007745Z","shell.execute_reply":"2022-06-30T09:10:13.020463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nimport pickle\n# 学習を実行する関数\ndef rank_trainer_dual(feature_df, feat_cols, label_col, fold_params, seed=2022):\n    scores = []\n    top_models = []\n    bottom_models = []\n    params = []\n\n    for fold_id, param in enumerate(fold_params):\n        ################################\n        # データ準備\n        ################################\n        train = feature_df[(param[0] <= feature_df['Date']) & (feature_df['Date'] < param[1])]\n        valid = feature_df[(param[1] <= feature_df['Date']) & (feature_df['Date'] < param[2])]\n        train = add_rank(train, col_name='Target', ascend=True)\n        train = add_bottom_rank(train, col_name='Target')\n        valid = add_rank(valid, col_name='Target', ascend=True)\n        valid = add_bottom_rank(valid, col_name='Target')\n        train = train.sort_values(['Date', 'SecuritiesCode']).reset_index(drop=True)\n        train_group = train.groupby('Date')['SecuritiesCode'].count().to_list()\n        valid = valid.sort_values(['Date', 'SecuritiesCode']).reset_index(drop=True)\n        valid_group = valid.groupby('Date')['SecuritiesCode'].count().to_list()\n        \n        \n\n        X_train = train[feat_cols]\n        y_train = train['Rank']\n        X_valid = valid[feat_cols]\n        y_valid = valid['Rank']\n        \n        lgb_train = lgb.Dataset(X_train, y_train, group=train_group)\n        lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train, group=valid_group)\n        \n        ################################\n        # 学習\n        ################################\n        label_gain = np.arange(max(max(train_group), max(valid_group))+100)\n        params = {\n#             'device': 'gpu',\n            'task': 'train',                   # 学習\n            'boosting_type': 'gbdt',           # GBDT\n            'objective': 'lambdarank',         # 回帰\n            'metric': 'ndcg',                  # 損失（誤差）\n            'label_gain':label_gain,\n#             'lambdarank_truncation_level':max(max(train_group), max(valid_group)),\n            'lambdarank_truncation_level':400,\n            'ndcg_eval_at':[200, 10, 50, 100, 150],\n            'learning_rate': 0.01,             # 学習率\n            'lambda_l1': 0.7,                  # L1正則化項の係数\n            'lambda_l2': 0.7,                  # L2正則化項の係数\n            'num_leaves': 31,                  # 最大葉枚数\n            'max_depth':7,\n#             'feature_fraction': 0.5,           # ランダムに抽出される列の割合\n            'bagging_fraction': 0.65,\n            'bagging_freq': 1, \n            'colsample_bytree': 0.7,\n            'colsample_bynode': 0.7,\n            'min_child_samples': 5,           # 葉に含まれる最小データ数\n            'force_col_wise': True,\n            'seed': seed                       # シード値\n        } \n \n        lgb_results = {}                       \n        model = lgb.train( \n            params,                            # ハイパーパラメータ\n            lgb_train,                         # 訓練データ\n            valid_sets=[lgb_train, lgb_valid], # 検証データ\n            valid_names=['Train', 'Valid'],    # データセット名前\n            num_boost_round=1000,              # 計算回数\n            early_stopping_rounds=-1,         # 計算打ち切り設定\n            evals_result=lgb_results,          # 学習の履歴\n            verbose_eval=100,                  # 学習過程の表示サイクル\n        )  \n\n        ################################\n        # 結果描画\n        ################################\n        fig = plt.figure(figsize=(10, 8))\n\n        # loss\n#         plt.subplot(1,2,1)\n#         loss_train = lgb_results['Train']['rho']\n#         loss_test = lgb_results['Valid']['rho']   \n#         plt.xlabel('Iteration')\n#         plt.ylabel('logloss')\n#         plt.plot(loss_train, label='train loss')\n#         plt.plot(loss_test, label='valid loss')\n#         plt.legend()\n\n        # feature importance\n        plt.subplot(1,2,2)\n        importance = pd.DataFrame({'feature':feat_cols, 'importance':model.feature_importance()})\n        sns.barplot(x = 'importance', y = 'feature', data = importance.sort_values('importance', ascending=False))\n\n        plt.tight_layout()\n        plt.show()\n\n        \n        # 推論\n        pred = model.predict(X_valid, num_iteration=model.best_iteration)\n        valid['TopPred'] = pred\n        \n        top_models.append(model)\n        model_name = f'lgb_top_rank_fold_{fold_id}.bin'\n        pickle.dump(model, open(model_name, 'wb'))\n        \n        ################################\n        # 下位200位の予測モデル\n        ################################\n        \n        X_train = train[feat_cols]\n        y_train = train['BottomRank']\n        X_valid = valid[feat_cols]\n        y_valid = valid['BottomRank']\n        \n        lgb_train = lgb.Dataset(X_train, y_train, group=train_group)\n        lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train, group=valid_group)\n        lgb_results = {} \n        model = lgb.train( \n            params,                            # ハイパーパラメータ\n            lgb_train,                         # 訓練データ\n            valid_sets=[lgb_train, lgb_valid], # 検証データ\n            valid_names=['Train', 'Valid'],    # データセット名前\n            num_boost_round=1000,              # 計算回数\n            early_stopping_rounds=-1,         # 計算打ち切り設定\n            evals_result=lgb_results,          # 学習の履歴\n            verbose_eval=100,                  # 学習過程の表示サイクル\n        )  \n\n        ################################\n        # 結果描画\n        ################################\n        fig = plt.figure(figsize=(10, 8))\n\n        # loss\n#         plt.subplot(1,2,1)\n#         loss_train = lgb_results['Train']['rho']\n#         loss_test = lgb_results['Valid']['rho']   \n#         plt.xlabel('Iteration')\n#         plt.ylabel('logloss')\n#         plt.plot(loss_train, label='train loss')\n#         plt.plot(loss_test, label='valid loss')\n#         plt.legend()\n\n        # feature importance\n        plt.subplot(1,2,2)\n        importance = pd.DataFrame({'feature':feat_cols, 'importance':model.feature_importance()})\n        sns.barplot(x = 'importance', y = 'feature', data = importance.sort_values('importance', ascending=False))\n\n        plt.tight_layout()\n        plt.show()\n        \n        \n#         # 推論\n        pred =  model.predict(X_valid, num_iteration=model.best_iteration)\n        valid['BottomPred'] = pred\n        \n        bottom_models.append(model)\n        model_name = f'lgb_bottom_rank_fold_{fold_id}.bin'\n        pickle.dump(model, open(model_name, 'wb'))\n        \n        \n        \n        ################################\n        # 評価\n        ################################\n        \n        valid = add_rank_with_new_col_name(valid, col_name='TopPred', new_col_name='TopPredRank', ascend=False)\n        valid = add_rank_with_new_col_name(valid, col_name='BottomPred', new_col_name='BottomPredRank', ascend=False)\n        \n        valid = add_preliminary_rank(valid)\n        valid = add_rank_with_new_col_name(valid, col_name='PreliminaryRank', new_col_name='SubmitRank', ascend=False)\n        \n        \n        # 評価\n        score = evaluator(valid, valid['SubmitRank'].to_numpy())\n\n        scores.append(score)\n\n    print(\"CV_SCORES:\", scores)\n    print(\"CV_SCORE:\", np.mean(scores))\n    \n    return top_models, bottom_models","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:20:58.723640Z","iopub.execute_input":"2022-06-30T09:20:58.724066Z","iopub.status.idle":"2022-06-30T09:20:58.761854Z","shell.execute_reply.started":"2022-06-30T09:20:58.723997Z","shell.execute_reply":"2022-06-30T09:20:58.760692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exec","metadata":{}},{"cell_type":"code","source":"# 2020-12-23よりも前のデータは証券コードが2000個すべて揃っていないため、これ以降のデータのみを使う。\n# (学習用データの開始日、学習用データの終了日＝検証用データの開始日、検証用データの終了日)\nfold_params = [\n    ('2018-04-01', '2020-05-01', '2020-06-01'),\n    ('2019-04-01', '2021-05-01', '2021-06-01'),\n    ('2020-04-01', '2022-05-01', '2022-06-01'),\n]\n# models = trainer(feature_df, feat_cols, label_col, fold_params)\n# models = rank_trainer(feature_df, feat_cols, label_col, fold_params)\ntop_models, bottom_models = rank_trainer_dual(feature_df, feat_cols, label_col, fold_params)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:51:41.741801Z","iopub.execute_input":"2022-06-30T09:51:41.742692Z","iopub.status.idle":"2022-06-30T11:49:57.104494Z","shell.execute_reply.started":"2022-06-30T09:51:41.742651Z","shell.execute_reply":"2022-06-30T11:49:57.103479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 推論・評価","metadata":{}},{"cell_type":"markdown","source":"生成したモデルを使って試験用データの推論を行いスコアを算出します。","metadata":{}},{"cell_type":"markdown","source":"## Function","metadata":{}},{"cell_type":"code","source":"def predictor(feature_df, feat_cols, models, is_train=True):\n    X = feature_df[feat_cols]\n    \n    # 推論\n    preds = list(map(lambda model: model.predict(X, num_iteration=model.best_iteration), models))\n    \n    # スコアは学習時のみ計算\n    if is_train:\n        scores = list(map(lambda pred: evaluator(feature_df, pred), preds))\n        print(\"SCORES:\", scores)\n\n    # 推論結果をバギング\n    pred = np.array(preds).mean(axis=0)\n    \n\n    # スコアは学習時のみ計算\n    if is_train:\n        score = evaluator(feature_df, pred)\n        print(\"SCORE:\", score)\n    \n    return pred","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:51:02.856770Z","iopub.execute_input":"2022-06-30T11:51:02.857191Z","iopub.status.idle":"2022-06-30T11:51:02.867906Z","shell.execute_reply.started":"2022-06-30T11:51:02.857143Z","shell.execute_reply":"2022-06-30T11:51:02.866879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictor_dual(feature_df, feat_cols, top_models, bottom_models, is_train=True):\n    X = feature_df[['Date', 'SecuritiesCode'] + feat_cols]\n    \n    # 推論\n    top_preds = list(map(lambda model: model.predict(X[feat_cols], num_iteration=model.best_iteration), top_models))\n    bottom_preds = list(map(lambda model: model.predict(X[feat_cols], num_iteration=model.best_iteration), bottom_models))\n    preds = []\n    for i in range(len(top_preds)):\n        X['TopPred'] = top_preds[i]\n        X['BottomPred'] = bottom_preds[i]\n        X = add_rank_with_new_col_name(X, col_name='TopPred', new_col_name='TopPredRank', ascend=False)\n        X = add_rank_with_new_col_name(X, col_name='BottomPred', new_col_name='BottomPredRank', ascend=False)\n        X = add_preliminary_rank(X)\n        X = add_rank_with_new_col_name(X, col_name='PreliminaryRank', new_col_name='SubmitRank', ascend=False)\n        preds.append(X['SubmitRank'].to_numpy())\n    # スコアは学習時のみ計算\n    if is_train:\n        scores = list(map(lambda pred: evaluator(feature_df, pred), preds))\n        print(\"SCORES:\", scores)\n\n    # 推論結果をバギング\n#     pred = np.array(preds).mean(axis=0)\n    w = np.array([0.2, 0.2, 0.6])\n    pred = np.average(np.array(preds), axis=0, weights=w)\n\n    # スコアは学習時のみ計算\n    if is_train:\n        score = evaluator(feature_df, pred)\n        print(\"SCORE:\", score)\n    \n    return pred","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:52:36.196165Z","iopub.execute_input":"2022-06-30T11:52:36.196710Z","iopub.status.idle":"2022-06-30T11:52:36.208855Z","shell.execute_reply.started":"2022-06-30T11:52:36.196672Z","shell.execute_reply":"2022-06-30T11:52:36.207738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exec","metadata":{}},{"cell_type":"code","source":"# 試験用データは学習用にも検証用にも使用していないものを使う\ntest_df = feature_df[('2022-02-01' <= feature_df['Date'])].copy()\n# pred = predictor(test_df, feat_cols, models)\npred = predictor_dual(test_df, feat_cols, top_models, bottom_models)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:52:39.409277Z","iopub.execute_input":"2022-06-30T11:52:39.410122Z","iopub.status.idle":"2022-06-30T11:53:22.601349Z","shell.execute_reply.started":"2022-06-30T11:52:39.410074Z","shell.execute_reply":"2022-06-30T11:53:22.600519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 提出","metadata":{}},{"cell_type":"markdown","source":"時系列APIを使って推論結果を登録します。  \n特徴量として、移動平均等の過去データを参照する値を採用しているため、時系列APIから得られたデータをため込む仕組みを実装する必要があります。  \nこの仕組みに関しては、[Start-to-finish demo based on s-meitoma + tweaks](https://www.kaggle.com/code/lowellniles/start-to-finish-demo-based-on-s-meitoma-tweaks)を参考にさせていただきました。  \n以下のコードでは`past_df`というデータフレームに履歴データをため込む実装になっています。","metadata":{}},{"cell_type":"code","source":"# 時系列APIのロード\nimport jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:10:14.581257Z","iopub.status.idle":"2022-06-30T09:10:14.581596Z","shell.execute_reply.started":"2022-06-30T09:10:14.581413Z","shell.execute_reply":"2022-06-30T09:10:14.581434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# supplemental filesを履歴データの初期状態としてセットアップ\npast_df = base_df[base_df['Date'] >= '2020-12-23'].copy()\npast_df = past_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:10:14.583143Z","iopub.status.idle":"2022-06-30T09:10:14.583484Z","shell.execute_reply.started":"2022-06-30T09:10:14.583309Z","shell.execute_reply":"2022-06-30T09:10:14.583332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 日次で推論・登録\nfor i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(iter_test):\n    print(f'iteration {i}')\n    current_date = prices[\"Date\"].iloc[0]\n\n    if i == 0:\n        # リークを防止するため、時系列APIから受け取ったデータより未来のデータを削除\n        past_df = past_df[past_df[\"Date\"] < current_date]\n\n#     # リソース確保のため古い履歴を削除\n#     threshold = (pd.Timestamp(current_date) - pd.offsets.BDay(80)).strftime(\"%Y-%m-%d\")\n#     past_df = past_df[past_df[\"Date\"] >= threshold]\n    \n    # 時系列APIから受け取ったデータを履歴データに統合\n    base_df = collector(prices, options, financials, trades, secondary_prices, stock_list)\n    past_df = pd.concat([past_df, base_df]).reset_index(drop=True)\n\n    # 特徴量エンジニアリング\n    feature_df, feat_cols, label_col = preprocessor(past_df, False)\n\n    # 予測対象レコードだけを抽出\n    feature_df = feature_df[feature_df['Date'] == current_date]\n\n    # 推論\n#     feature_df[\"pred\"] = predictor(feature_df, feat_cols, models, False)\n    feature_df['pred'] = predictor_dual(feature_df, feat_cols, top_models, bottom_models, False)\n\n    # 推論結果からRANKを導出し、提出データに反映\n    feature_df = add_rank(feature_df)\n    feature_map = feature_df.set_index('SecuritiesCode')['Rank'].to_dict()\n    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(feature_map)\n    \n\n    # 結果を登録\n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:10:14.585001Z","iopub.status.idle":"2022-06-30T09:10:14.585370Z","shell.execute_reply.started":"2022-06-30T09:10:14.585193Z","shell.execute_reply":"2022-06-30T09:10:14.585212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}