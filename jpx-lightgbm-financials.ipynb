{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"https://www.kaggle.com/code/ikeppyo/jpx-lightgbm-demo をベースにして作成した。","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# コンテストの概要\n* 日本取引所グループ（JPX）による、2000銘柄の証券を対象にした、**証券の値段（終値）の変化率**予測。\n* **翌日から翌々日にかけての終値**の変化率が目的変数。\n* 提出データは、目的変数の値そのものではなく、目的変数の値を降順に並べた際の順位。\n* コンテストについて、より詳しく知りたい場合は[【日本語ver】Easy to understand the competition](https://www.kaggle.com/code/chumajin/ver-easy-to-understand-the-competition)が非常に役に立つと思います。","metadata":{}},{"cell_type":"markdown","source":"# ノートブックの概要\n* このノートブックでは[データ読み込み]→[データ統合]→[特徴量エンジニアリング]→[学習]→[推論・評価]→[提出]を一気通貫で行います。\n* 使用するモデルはLightGBMです。\n* モデルを3つ生成し、結果をアンサンブルして最終的な推論結果を作成します。\n\n推論・評価までの流れは以下の通りです。  \n赤背景は推論・評価推時のみ使用している関数です。青背景は提出時にも使用している関数・データとなります。  \n青背景の関数をカスタマイズすることで、様々な特徴量で精度検証ができ、また、そのままSubmitもできるようになっています。","metadata":{}},{"cell_type":"code","source":"# Image('../input/jpx-images/jpx_flow.drawio.png')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:40:41.528662Z","iopub.execute_input":"2022-06-30T13:40:41.529170Z","iopub.status.idle":"2022-06-30T13:40:41.533321Z","shell.execute_reply.started":"2022-06-30T13:40:41.529141Z","shell.execute_reply":"2022-06-30T13:40:41.532029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 入力ファイルの概要\n* `jpx-tokyo-stock-exchange-prediction/`\n    * `stock_list.csv`:  主な項目は証券コードと証券に関する属性情報。Keyは**SecuritiesCode**。\n    * `train_files/`:  学習用データ群。Dateの範囲は2017-01-04 ～ 2021-12-03。\n        * `stock_prices.csv`:  主な項目は証券コードと日単位の証券価格、および、目的変数（Target）。Keyは**Date**と**SecuritiesCode**であり、これらを連結した**RowId**も用意されている。\n        * `secondary_stock_prices.csv`:  項目は**stock_prices.csv**と同じだが、**stock_prices.csv**の対象とならなかった証券が入っている。Keyは**stock_prices.csv**と同じ。\n        * `options.csv`:  主な項目はオプション（証券用語）コードと日単位のオプション価格。Keyは**Date**と**OptionsCode**であり、これらを連結した**DateCode**も用意されている。\n        * `trades.csv`:  主な項目はマーケット毎の前営業週における取引サマリ。Keyは**Date**と**Section**。**Section**を加工することで**stock_list.csv**の**NewMarketSegment**と紐づけることができる。意味のあるレコードの発生は週次。\n        * `financials.csv`:  主な項目は四半期決算報告の内容。Keyは**Date**と**SecuritiesCode**であり、これらを連結した**DateCode**も用意されている。レコードの発生は四半期毎。\n    * `supplemental_files/`:  追加の学習用データ群。5月上旬、6月上旬、およびコンテスト終了直前に最新のデータが反映される。2022-04-05時点でのDateの範囲は2021-12-06 ～ 2022-02-28。\n        * **train_files**配下と同じ形式のファイル群が格納されている\n        * `example_test_files/`:  評価用データ群（のサンプル）\n        * **train_files**配下とほぼ（※）同じ形式のファイル群が格納されている\n        * ※ **stock_prices.csv**と**secondary_stock_prices.csv**から**Target**が削除されている点だけが**train_files**配下と異なる\n    * `data_specifications/`:  上記ファイル群の項目説明書\n        * `stock_list_spec.csv`:  **stock_list.csv**の項目説明書\n        * `stock_prices_spec.csv`:  **stock_prices.csv**、**secondary_stock_prices.csv**の項目説明書\n        * `options_spec.csv`:  **options.csv**の項目説明書\n        * `trades_spec.csv`:  **trades.csv**の項目説明書\n        * `stock_fin_spec.csv`:  **financials.csv**の項目説明書\n    * `jpx_tokyo_market_prediction/`:時系列API\n    \nファイル間の関係性は以下のようなイメージです","metadata":{}},{"cell_type":"code","source":"# Image('../input/jpx-images/jpx_files.drawio.png')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:40:41.541858Z","iopub.execute_input":"2022-06-30T13:40:41.542223Z","iopub.status.idle":"2022-06-30T13:40:41.547303Z","shell.execute_reply.started":"2022-06-30T13:40:41.542186Z","shell.execute_reply":"2022-06-30T13:40:41.546441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"データの詳細は https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction/data を参照  \n時系列APIの使い方は https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction/overview/evaluation を参照  \nオプションコード体系の日本語版は https://www.jpx.co.jp/sicc/securities-code/nlsgeu0000032d48-att/(HP)sakimono20220208.pdf を参照  \n証券コード関係は https://www.jpx.co.jp/sicc/securities-code/01.html を参照","metadata":{}},{"cell_type":"markdown","source":"# 準備\n* 使用するライブラリをインポートします","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nfrom pathlib import Path\nfrom decimal import ROUND_HALF_UP, Decimal\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:40:41.549545Z","iopub.execute_input":"2022-06-30T13:40:41.549921Z","iopub.status.idle":"2022-06-30T13:40:42.603106Z","shell.execute_reply.started":"2022-06-30T13:40:41.549887Z","shell.execute_reply":"2022-06-30T13:40:42.602321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if (c_min > np.iinfo(np.int8).min) & (c_max < np.iinfo(np.int8).max):\n                    df[col] = df[col].astype(np.int8)\n                elif (c_min > np.iinfo(np.int16).min) & (c_max < np.iinfo(np.int16).max):\n                    df[col] = df[col].astype(np.int16)\n                elif (c_min > np.iinfo(np.int32).min) & (c_max < np.iinfo(np.int32).max):\n                    df[col] = df[col].astype(np.int32)\n                elif (c_min > np.iinfo(np.int64).min) & (c_max < np.iinfo(np.int64).max):\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if (c_min > np.finfo(np.float16).min) & (c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > np.finfo(np.float32).min) & (c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:40:42.604567Z","iopub.execute_input":"2022-06-30T13:40:42.604862Z","iopub.status.idle":"2022-06-30T13:40:42.620714Z","shell.execute_reply.started":"2022-06-30T13:40:42.604825Z","shell.execute_reply":"2022-06-30T13:40:42.620049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# データ読み込み\n\n以下のファイル群を読み込みます。\n* stock_list.csv\n* train_files 配下のファイル（Dateの範囲は2017-01-04 ～ 2021-12-03）\n* supplemental_files 配下のファイル（Dateの範囲は2021-12-06 ～ 2022-02-28 ※2022/04/05現在）","metadata":{}},{"cell_type":"markdown","source":"## Function","metadata":{}},{"cell_type":"code","source":"def read_files(dir_name: str = 'train_files'):\n    base_path = Path(f'../input/jpx-tokyo-stock-exchange-prediction/{dir_name}')\n    prices = pd.read_csv(base_path / 'stock_prices.csv')\n    options = pd.read_csv(base_path / 'options.csv')\n    financials = pd.read_csv(base_path / 'financials.csv')\n    trades = pd.read_csv(base_path / 'trades.csv')\n    secondary_prices = pd.read_csv(base_path / 'secondary_stock_prices.csv')\n    \n    prices = reduce_mem_usage(prices)\n    options = reduce_mem_usage(options)\n    financials = reduce_mem_usage(financials)\n    trades = reduce_mem_usage(trades)\n    secondary_prices = reduce_mem_usage(secondary_prices)\n    \n    return prices, options, financials, trades, secondary_prices","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:40:42.622097Z","iopub.execute_input":"2022-06-30T13:40:42.622357Z","iopub.status.idle":"2022-06-30T13:40:42.630795Z","shell.execute_reply.started":"2022-06-30T13:40:42.622321Z","shell.execute_reply":"2022-06-30T13:40:42.630063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exec","metadata":{}},{"cell_type":"code","source":"%%time\n\nstock_list = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv')\ntrain_files = read_files('train_files')\nsupplemental_files = read_files('supplemental_files')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:40:42.632321Z","iopub.execute_input":"2022-06-30T13:40:42.632614Z","iopub.status.idle":"2022-06-30T13:41:21.202886Z","shell.execute_reply.started":"2022-06-30T13:40:42.632579Z","shell.execute_reply":"2022-06-30T13:41:21.201432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# データ統合\n\n* 読み込んだデータを結合して、一つのファイルに纏めます。","metadata":{}},{"cell_type":"code","source":"train_files[2]['NetSales'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:41:21.204391Z","iopub.execute_input":"2022-06-30T13:41:21.204629Z","iopub.status.idle":"2022-06-30T13:41:21.226723Z","shell.execute_reply.started":"2022-06-30T13:41:21.204596Z","shell.execute_reply":"2022-06-30T13:41:21.225940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Function","metadata":{}},{"cell_type":"markdown","source":"`merge_data`関数では各ファイルを水平方向に結合します。現時点では`stock_prices`、`stock_list`しか使っていません。  \nコメントアウトを解除すれば`trades`、`financials`とも結合は可能ですが、  \nこれらのデータは、有効なレコードが発生するタイミングが日次ではないため、学習データとして意味のあるものとするためには「直近に発生した有効なレコードの値を引き継ぐ」などの対処が必要となります。  \n`options`に関しては、[OptionsCodeの附番規則](https://www.jpx.co.jp/sicc/securities-code/nlsgeu0000032d48-att/(HP)sakimono20220208.pdf)を見ていけば適切な使い方が見えそうです。","metadata":{}},{"cell_type":"code","source":"def merge_data(prices, options, financials, trades, secondary_prices, stock_list):\n    # stock_prices がベース\n    base_df = prices.copy()\n    \n    # stock_listと結合\n    _stock_list = stock_list.copy()\n    _stock_list.rename(columns={'Close': 'Close_x'}, inplace=True)\n    base_df = base_df.merge(_stock_list, on='SecuritiesCode', how=\"left\")\n\n    # tradesと結合\n    # stock_listのNewMarketSegmentと紐づくよう、tradesのSection項目を編集する\n    # _trades = trades.copy()\n    # _trades['NewMarketSegment'] = _trades['Section'].str.split(' \\(', expand=True)[0]\n    # base_df = base_df.merge(_trades, on=['Date', 'NewMarketSegment'], how=\"left\")\n\n    # financialsと結合\n    _financials = financials.copy()\n    _financials.rename(columns={'Date': 'Date_x', 'SecuritiesCode': 'SecuritiesCode_x'}, inplace=True)\n    use_cols = ['DateCode',\n               'NetSales',\n               'OperatingProfit',\n               'OrdinaryProfit',\n               'Profit', \n               'EarningsPerShare', \n               'TotalAssets', \n               'Equity', \n               'EquityToAssetRatio', \n               'NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock',\n               'ForecastEarningsPerShare']\n    # メモリがカツカツなのでカラムを使うものだけに絞る\n    _financials = _financials[use_cols]\n    _financials.replace(['-', '－'], np.nan, inplace=True)\n    for col in use_cols[1:]:\n        _financials[col] = _financials[col].astype(float)\n    prev_column = set(_financials.columns)\n    \n    # 同じDateCodeで複数行あるのでまとめあげる\n    _financials = _financials.groupby('DateCode').max().reset_index()\n    assert prev_column == set(_financials.columns.to_list())\n    _financials.drop_duplicates(subset='DateCode', inplace=True)\n    assert prev_column == set(_financials.columns.to_list())\n    base_df = base_df.merge(_financials, left_on='RowId', right_on='DateCode', how=\"left\")\n    \n    return base_df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:41:21.229821Z","iopub.execute_input":"2022-06-30T13:41:21.230096Z","iopub.status.idle":"2022-06-30T13:41:21.241997Z","shell.execute_reply.started":"2022-06-30T13:41:21.230062Z","shell.execute_reply":"2022-06-30T13:41:21.241109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`adjust_price`関数は[Train Demo](https://www.kaggle.com/code/smeitoma/train-demo)で紹介されている関数をほぼそのまま使わせて頂いています。（Dateのインデックス化だけコメントアウトしています）  \n項目追加が発生するため、統合の範囲を超えていますが、関数内でソートやインデックスの生成といった操作が行われるため、この段階で実行しています。  \n  \n関数では**AdjustedClose**という項目が生成されます。  \n株式は、分割や併合によって株価が大きく変動することがありますが、**Close**の代わりに**AdjustedClose**を使うことで、この影響を減少させることができるとのことです。","metadata":{}},{"cell_type":"code","source":"def adjust_price(price):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n    # transform Date column into datetime\n    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n\n    def generate_adjusted_close(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n\n    # price.set_index(\"Date\", inplace=True)\n    return price","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:41:21.243299Z","iopub.execute_input":"2022-06-30T13:41:21.243970Z","iopub.status.idle":"2022-06-30T13:41:21.255049Z","shell.execute_reply.started":"2022-06-30T13:41:21.243934Z","shell.execute_reply":"2022-06-30T13:41:21.254175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collector(prices, options, financials, trades, secondary_prices, stock_list):\n    # 読み込んだデータを統合して一つのファイルに纏める\n    base_df = merge_data(prices, options, financials, trades, secondary_prices, stock_list)\n    \n    # AdjustedClose項目の生成\n    base_df = adjust_price(base_df)\n    \n    return base_df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:41:21.256390Z","iopub.execute_input":"2022-06-30T13:41:21.256771Z","iopub.status.idle":"2022-06-30T13:41:21.266203Z","shell.execute_reply.started":"2022-06-30T13:41:21.256735Z","shell.execute_reply":"2022-06-30T13:41:21.265390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exec","metadata":{}},{"cell_type":"markdown","source":"supplemental filesを使わない場合は4行目以降をコメントアウトします。","metadata":{}},{"cell_type":"code","source":"%%time\n\nbase_df = collector(*train_files, stock_list)\nsupplemental_df = collector(*supplemental_files, stock_list)\nbase_df = pd.concat([base_df, supplemental_df]).reset_index(drop=True)\n\nbase_df['Target'] = base_df['Target'] * 100","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:41:21.267562Z","iopub.execute_input":"2022-06-30T13:41:21.267995Z","iopub.status.idle":"2022-06-30T13:42:03.073045Z","shell.execute_reply.started":"2022-06-30T13:41:21.267957Z","shell.execute_reply":"2022-06-30T13:42:03.072290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:42:03.074393Z","iopub.execute_input":"2022-06-30T13:42:03.074794Z","iopub.status.idle":"2022-06-30T13:42:03.175987Z","shell.execute_reply.started":"2022-06-30T13:42:03.074756Z","shell.execute_reply":"2022-06-30T13:42:03.175256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:42:03.177320Z","iopub.execute_input":"2022-06-30T13:42:03.177581Z","iopub.status.idle":"2022-06-30T13:42:03.234442Z","shell.execute_reply.started":"2022-06-30T13:42:03.177547Z","shell.execute_reply":"2022-06-30T13:42:03.233806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 特徴量エンジニアリング\n\n特徴量を生成し、推論結果の精度向上に貢献するものだけを選びます。  ","metadata":{}},{"cell_type":"markdown","source":"## Function\n\n`calc_change_rate_base`関数と`calc_volatility_base`関数は、[Train Demo](https://www.kaggle.com/code/smeitoma/train-demo)で紹介されていた関数を参考にしています。","metadata":{}},{"cell_type":"code","source":"def add_date_features(df):\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n#     df[\"Year\"] = df[\"Date\"].dt.year.astype(\"int32\")\n    df['Month'] = df[\"Date\"].dt.month.astype(\"int8\")\n    df['day_of_month'] = df[\"Date\"].dt.day.astype(\"int8\")\n    df['day_of_year'] = df[\"Date\"].dt.dayofyear.astype(\"int16\")\n    df['day_of_week'] = (df[\"Date\"].dt.dayofweek + 1).astype(\"int8\")\n    return df\n\n# 「column_name で指定された項目の、periodsで指定された期間（複数）での変化率を導出し、項目として追加する関数」を生成する関数\n# 生成された関数は、特定証券コードだけを持つデータフレームが入力されることを前提としている。\ndef calc_change_rate_base(column_name, periods):\n    def func(price):\n        for period in periods:\n            price.loc[:, f\"{column_name}_change_rate_{period}\"] = price[column_name].pct_change(period)\n        return price\n    return func\n\n\n# 「column_name で指定された項目の、periodsで指定された期間（複数）での変動の度合いを導出し、項目として追加する関数」を生成する関数\n# 生成された関数は、特定証券コードだけを持つデータフレームが入力されることを前提としている。\ndef calc_volatility_base(column_name, periods):\n    def func(price):\n        for period in periods:\n            price.loc[:, f\"{column_name}_volatility_{period}\"] = np.log(price[column_name]).diff().rolling(window=period, min_periods=1).std()\n        return price\n    return func\n\n# 「column_name で指定された項目の、periodsで指定された期間（複数）での移動平均値と現在値の比率を導出し、項目として追加する関数」を生成する関数\n# 移動平均値そのものではなく、現在値に対する比率としているのは、今回のTargetが比率であるため。\n# 生成された関数は、特定証券コードだけを持つデータフレームが入力されることを前提としている。\ndef calc_moving_average_rate_base(column_name, periods):\n    def func(price):\n        for period in periods:\n            price.loc[:, f\"{column_name}_average_rate_{period}\"] = price[column_name].rolling(window=period, min_periods=1).mean() / price[column_name]\n            price.loc[:, f\"{column_name}_average_deviation_rate_{period}\"] = (price[column_name] - price[column_name].rolling(window=period, min_periods=1).mean()) / price[column_name]\n        short_period = periods[0]\n        mid_period = periods[1]\n        s = price[f\"{column_name}_average_rate_{short_period}\"] - price[f\"{column_name}_average_rate_{mid_period}\"]\n        s = s / s.abs()\n        price.loc[:, 'moving_average_cross'] = s - s.shift(1)\n\n        return price\n    return func\n\ndef calc_rolling_features_base(column_name, periods):\n    def func(price):\n        for period in periods:\n            price.loc[:, f\"{column_name}_rolling_min_{period}\"] = price[column_name].rolling(window=period, min_periods=1).min() / price[column_name]\n            price.loc[:, f\"{column_name}_rolling_max_{period}\"] = price[column_name].rolling(window=period, min_periods=1).max() / price[column_name]\n        return price\n    return func\n\n\n# # 終値の変動率を生成し、項目として追加する関数。これをShift-2するとTargetになる。\n# # この関数は、特定証券コードだけを持つデータフレームが入力されることを前提としている。\n# def calc_target_shift2(price):\n#     price.loc[:, 'Target_shift2'] = price['Close'].pct_change()\n#     return price\n\ndef calc_target_shifts(price):\n    lags = [1, 2, 3, 4, 5, 6, 7]\n    for i in lags:\n        price.loc[:, f'Target_shift{i}'] = price['Target'].shift(i)\n    return price\n\ndef calc_33Sector_rolling_features(base_df, periods):\n    # 33SectorCodeの平均リターンを使った特徴量\n    base_df['return'] = base_df.groupby('SecuritiesCode')['AdjustedClose'].pct_change()\n    base_df2 = base_df.copy()\n    sector_df = base_df2.groupby(['33SectorCode', 'Date'])['return'].mean().reset_index()\n    for period in periods:\n        sector_df[f'33Sector_rolling_mean_{period}'] = sector_df.groupby(['33SectorCode'])['return'].rolling(window=period).mean().reset_index()['return']\n        \n    base_df = base_df.merge(sector_df[['33SectorCode', 'Date', '33Sector_rolling_mean_5', '33Sector_rolling_mean_20', '33Sector_rolling_mean_80']],\n                            on=['33SectorCode', 'Date'],\n                           how='left')\n    # returnが0のときに発散するのを防ぐために下駄を履かせる\n    constant = abs(base_df['return'].min()) + 0.001\n    for period in periods:\n        base_df[f'deviation_from_sector_MA_{period}'] = (base_df['return'] - base_df[f'33Sector_rolling_mean_{period}']) / (base_df['return'] + constant)\n    \n    base_df.drop('return', axis=1, inplace=True)\n    return base_df\n\n#financialsデータを使った特徴量\ndef add_fin_features(price):\n#     def func(price):\n    # ここで行うのは微妙だが良さそうな場所もないので、ここでfinancialsのffillをやる。\n    f_columns = ['NetSales',\n               'OperatingProfit',\n               'OrdinaryProfit',\n               'Profit', \n               'EarningsPerShare', \n               'TotalAssets', \n               'Equity', \n               'EquityToAssetRatio', \n               'NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock']\n    for column in f_columns:\n        price[column].fillna(method='ffill', inplace=True)\n    #自己資本(CapitalAdequacy)の計算\n    price.loc[:, 'TotalAssets']=pd.to_numeric(price.loc[:, 'TotalAssets'], errors=\"coerce\")\n    price.loc[:, 'EquityToAssetRatio']=pd.to_numeric(price.loc[:, 'EquityToAssetRatio'], errors=\"coerce\")\n    price.loc[:, \"CapitalAdequacy\"]=price.loc[:,'EquityToAssetRatio']*price.loc[:,'TotalAssets']\n#     base_df[\"CapitalAdequacy\"].fillna(method='ffill', inplace=True)\n    #時価総額(AMV)の計算\n    price.loc[:, \"NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock\"]=pd.to_numeric(price.loc[:, 'NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock'], errors=\"coerce\")\n    price.loc[:, \"AMV\"]=price.loc[:,'NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock']*price.loc[:,\"Close\"]\n#     base_df[\"AMV\"].fillna(method='ffill', inplace=True)\n    #時価簿価比率:簿価(総資産)と時価の差分を表す．この差分は、投資家の企業の将来に対する期待と解釈される\n    price.loc[:, \"PBR\"]=price.loc[:,\"AMV\"]/price.loc[:,'CapitalAdequacy']\n#     base_df[\"PBR\"].fillna(method='ffill', inplace=True)\n    #負債時価総額比率:資金調達方法(負債と株式発行)の割合．値が大きいほど負債が多く、倒産リスクが大きくなる\n    price.loc[:, 'Equity']=pd.to_numeric(price.loc[:, 'Equity'], errors=\"coerce\")\n    price.loc[:, \"debt\"]=price.loc[:,\"TotalAssets\"]-price.loc[:,\"Equity\"]\n    price.loc[:, \"RatioOfNetWorthToTotalDebt\"]=price.loc[:, \"debt\"]/price.loc[:, 'AMV']\n#     base_df[\"RatioOfNetWorthToTotalDebt\"].fillna(method='ffill', inplace=True)\n    price.drop(\"debt\", axis=1, inplace=True)\n    #営業マージン:どの程度効率よく付加価値をつけているか\n    price.loc[:, \"OperatingProfit\"]=pd.to_numeric(price.loc[:, \"OperatingProfit\"], errors=\"coerce\")\n    price.loc[:, \"NetSales\"]=pd.to_numeric(price.loc[:, \"NetSales\"], errors=\"coerce\")\n    price.loc[:, \"OperatingMargin\"]=price.loc[:,\"OperatingProfit\"]/price.loc[:,\"NetSales\"]\n#     base_df[\"OperatingMargin\"].fillna(method='ffill', inplace=True)\n    #純利益マージン；営業マージンとほとんど同じ．違いは負債などに対する支払いの有無．つまり、資金調達の効率性も加味される\n    price.loc[:, \"Profit\"]=pd.to_numeric(price.loc[:, \"Profit\"], errors=\"coerce\")\n    price.loc[:, \"ProfitMargin\"]=price.loc[:,\"Profit\"]/price.loc[:,\"NetSales\"]\n#     base_df[\"ProfitMargin\"].fillna(method='ffill', inplace=True)\n    #自己資本利益率(ROE)\n    price.loc[:, \"ROE\"]=price.loc[:,\"Profit\"]/price.loc[:,\"Equity\"]\n#     base_df[\"ROE\"].fillna(method='ffill', inplace=True)\n    #総資産回転率:保有資産の効率性を表す\n    price.loc[:, \"TotalAssetsTurnover\"]=price.loc[:,\"NetSales\"]/price.loc[:,\"TotalAssets\"]\n#     base_df[\"TotalAssetsTurnover\"].fillna(method='ffill', inplace=True)\n    #自己資本倍率\n    price.loc[:, \"CapitalAdequecyRatio\"]=price.loc[:,\"Profit\"]/price.loc[:,\"TotalAssets\"]\n#     base_df[\"CapitalAdequecyRatio\"].fillna(method='ffill', inplace=True)\n\n    return price\n#     return func\n\n# 入力データフレームを証券コード毎にグルーピングし、引数で渡された関数を適用する関数\n# functionsには↑で定義したcalc_xxxの関数のリストが渡される想定。\ndef add_columns_per_code(price, functions):\n    def func(df):\n        for f in functions:\n            df = f(df)\n        return df\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(func)\n    price = price.reset_index(drop=True)\n    return price\n\n# 入力データフレームに特徴量を追加する関数\n# 追加する項目は、基本的にレコード内の値だけを使う想定\ndef add_columns_per_day(base_df):\n    base_df['diff_rate1'] = (base_df['Close'] - base_df['Open']) / base_df['Close']\n    base_df['diff_rate2'] = (base_df['High'] - base_df['Low']) / base_df['Close']    \n    return base_df\n\n# 入力データフレームに特徴量を追加する関数\ndef generate_features(base_df):\n    prev_column_names = base_df.columns\n    \n    periods = [5, 20, 80]\n    functions = [\n#         calc_change_rate_base(\"AdjustedClose\", periods), \n        calc_volatility_base(\"AdjustedClose\", periods), \n        calc_moving_average_rate_base(\"Volume\", periods), \n        calc_moving_average_rate_base(\"AdjustedClose\", periods), \n        calc_rolling_features_base(\"AdjustedClose\", periods),\n#         calc_target_shifts\n    ]\n    \n    # 日付の特徴量\n    base_df = add_date_features(base_df)\n    \n    # 33SectorCodeごとのリターンの移動平均\n    base_df = calc_33Sector_rolling_features(base_df, periods)\n    \n    # 財務諸表からの特徴量\n    base_df = base_df.sort_values([\"SecuritiesCode\", \"Date\"])\n    base_df = base_df.groupby(\"SecuritiesCode\").apply(add_fin_features)\n    base_df = base_df.reset_index(drop=True)\n#     base_df = add_fin_features(base_df)\n    \n    # 証券コード単位の特徴量（移動平均等、一定期間のレコードをインプットに生成する特徴量）を追加\n    base_df = add_columns_per_code(base_df, functions)\n    # 日単位の特徴量（レコード内の値で導出できる特徴量）を追加\n    base_df = add_columns_per_day(base_df)\n    \n    # 後で特徴量を選択しやすくするため、追加した項目名のリストを生成\n    add_column_names = list(set(base_df.columns) - set(prev_column_names))\n    return base_df, add_column_names","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:42:03.235849Z","iopub.execute_input":"2022-06-30T13:42:03.236255Z","iopub.status.idle":"2022-06-30T13:42:03.274582Z","shell.execute_reply.started":"2022-06-30T13:42:03.236222Z","shell.execute_reply":"2022-06-30T13:42:03.273908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 特徴量選択\ndef select_features(feature_df, add_column_names, is_train):\n    # 基本項目\n    base_cols = ['RowId', 'Date', 'SecuritiesCode']\n    # 数値系の特徴量\n    numerical_cols = sorted(add_column_names)\n    numerical_cols.remove('moving_average_cross')\n    # カテゴリ系の特徴量\n    categorical_cols = ['NewMarketSegment', '33SectorCode', '17SectorCode', 'moving_average_cross']\n    # 目的変数\n    label_col = ['Target']\n    \n    # 特徴量\n    feat_cols = numerical_cols + categorical_cols\n\n    # データフレームの項目を選択された項目だけに絞込\n    feature_df = feature_df[base_cols + feat_cols + label_col]\n    # カテゴリ系項目はdtypeをcategoryに変更\n    feature_df[categorical_cols] = feature_df[categorical_cols].astype('category')\n    \n    if is_train:\n        # 学習データの場合は、NA項目があるレコードを削除\n        feature_df.dropna(inplace=True)\n    else:\n        # 推論データの場合は、NA項目を補完\n        feature_df[numerical_cols] = feature_df[numerical_cols].fillna(-999)\n        feature_df[numerical_cols] = feature_df[numerical_cols].replace([np.inf, -np.inf], -999)\n    \n    return feature_df, feat_cols, label_col","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:42:03.275994Z","iopub.execute_input":"2022-06-30T13:42:03.276249Z","iopub.status.idle":"2022-06-30T13:42:03.286707Z","shell.execute_reply.started":"2022-06-30T13:42:03.276217Z","shell.execute_reply":"2022-06-30T13:42:03.285918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessor(base_df, is_train=True):\n    feature_df = base_df.copy()\n    \n    ## 特徴量生成\n    feature_df, add_column_names = generate_features(feature_df)\n    \n    ## 特徴量選択\n    feature_df, feat_cols, label_col = select_features(feature_df, add_column_names, is_train)\n    \n    feature_df = reduce_mem_usage(feature_df)\n\n    return feature_df, feat_cols, label_col","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:42:03.287921Z","iopub.execute_input":"2022-06-30T13:42:03.288338Z","iopub.status.idle":"2022-06-30T13:42:03.297978Z","shell.execute_reply.started":"2022-06-30T13:42:03.288292Z","shell.execute_reply":"2022-06-30T13:42:03.297256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exec","metadata":{}},{"cell_type":"code","source":"%%time\n\nfeature_df, feat_cols, label_col = preprocessor(base_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:42:03.299212Z","iopub.execute_input":"2022-06-30T13:42:03.299656Z","iopub.status.idle":"2022-06-30T13:43:47.249078Z","shell.execute_reply.started":"2022-06-30T13:42:03.299622Z","shell.execute_reply":"2022-06-30T13:43:47.248324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:43:47.250400Z","iopub.execute_input":"2022-06-30T13:43:47.250796Z","iopub.status.idle":"2022-06-30T13:43:47.351565Z","shell.execute_reply.started":"2022-06-30T13:43:47.250759Z","shell.execute_reply":"2022-06-30T13:43:47.350833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:43:47.352959Z","iopub.execute_input":"2022-06-30T13:43:47.353386Z","iopub.status.idle":"2022-06-30T13:43:47.422860Z","shell.execute_reply.started":"2022-06-30T13:43:47.353350Z","shell.execute_reply":"2022-06-30T13:43:47.422225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_cols","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:43:47.424085Z","iopub.execute_input":"2022-06-30T13:43:47.424341Z","iopub.status.idle":"2022-06-30T13:43:47.429970Z","shell.execute_reply.started":"2022-06-30T13:43:47.424308Z","shell.execute_reply":"2022-06-30T13:43:47.429223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 学習","metadata":{}},{"cell_type":"markdown","source":"## Function","metadata":{}},{"cell_type":"markdown","source":"学習を行いモデルを生成します","metadata":{}},{"cell_type":"code","source":"# 予測値を降順に並べて順位番号を振る関数\n# 言い換えると、目的変数から提出用項目を導出する関数\ndef add_rank(df, col_name=\"pred\"):\n    df[\"Rank\"] = df.groupby(\"Date\")[col_name].rank(ascending=False, method=\"first\") - 1 \n    df[\"Rank\"] = df[\"Rank\"].astype(\"int\")\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:43:47.431385Z","iopub.execute_input":"2022-06-30T13:43:47.431633Z","iopub.status.idle":"2022-06-30T13:43:47.437786Z","shell.execute_reply.started":"2022-06-30T13:43:47.431600Z","shell.execute_reply":"2022-06-30T13:43:47.437064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`calc_spread_return_sharpe`関数は[Train Demo](https://www.kaggle.com/code/smeitoma/train-demo)で紹介されている関数をそのまま使わせて頂いています。  \n推論した**Rank**と、正解の**Target**を含むデータフレームを渡すと、コンテストの評価方法に沿ったスコアを計算してくれます。","metadata":{}},{"cell_type":"code","source":"def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:43:47.442074Z","iopub.execute_input":"2022-06-30T13:43:47.442694Z","iopub.status.idle":"2022-06-30T13:43:47.451604Z","shell.execute_reply.started":"2022-06-30T13:43:47.442652Z","shell.execute_reply":"2022-06-30T13:43:47.450953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 予測用のデータフレームと、予測結果をもとに、スコアを計算する関数\ndef evaluator(df, pred):\n    df[\"pred\"] = pred\n    df = add_rank(df)\n    score = calc_spread_return_sharpe(df)\n    return score","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:43:47.452804Z","iopub.execute_input":"2022-06-30T13:43:47.453227Z","iopub.status.idle":"2022-06-30T13:43:47.460587Z","shell.execute_reply.started":"2022-06-30T13:43:47.453193Z","shell.execute_reply":"2022-06-30T13:43:47.459846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`lightgbm`ではなく`optuna.integration.lightgbm`をimportすることで、パイパーパラメータチューニングが実行されるようになります。","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\n# import optuna.integration.lightgbm as lgb\n\ndef rmspe_loss_lgb(y_pred, train_dataset):\n\n    \"\"\"\n    Calculate gradient and hessian of the loss function root mean squared percentage error\n\n    Parameters\n    ----------\n    y_pred [array-like of shape (n_samples)]: Predictions\n    train_dataset (lightgbm.basic.Dataset): Training dataset\n\n    Returns\n    -------\n    gradient (float): First order derivative of the loss function root mean squared percentage error\n    hessian (float): Second order derivative of the loss function root mean squared percentage error\n    \"\"\"\n\n    y_true = train_dataset.get_label()\n    gradient = 2.0 / (y_true+1) * ((y_pred+1) * 1.0 / (y_true+1) - 1)\n    hessian = 2.0 / ((y_true+1) ** 2)\n\n    return gradient, hessian\n    \n    \ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / (y_true)))))\n\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\n# 学習を実行する関数\ndef trainer(feature_df, feat_cols, label_col, fold_params, seed=2022):\n    scores = []\n    models = []\n    params = []\n\n    for param in fold_params:\n        ################################\n        # データ準備\n        ################################\n        train = feature_df[(param[0] <= feature_df['Date']) & (feature_df['Date'] < param[1])]\n        valid = feature_df[(param[1] <= feature_df['Date']) & (feature_df['Date'] < param[2])]\n\n        X_train = train[feat_cols]\n        y_train = train[label_col]\n        X_valid = valid[feat_cols]\n        y_valid = valid[label_col]\n        \n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n        \n        ################################\n        # 学習\n        ################################\n        params = {\n            'task': 'train',                   # 学習\n            'boosting_type': 'gbdt',           # GBDT\n            'objective': 'quantile',         # 回帰\n            'metric': 'rmse',                  # 損失（誤差）\n            'learning_rate': 0.01,             # 学習率\n            'lambda_l1': 0.5,                  # L1正則化項の係数\n            'lambda_l2': 0.5,                  # L2正則化項の係数\n            'num_leaves': 32,                  # 最大葉枚数\n            'max_depth':7,\n#             'feature_fraction': 0.5,           # ランダムに抽出される列の割合\n            'bagging_fraction': 0.65,\n            'bagging_freq': 1, \n            'colsample_bytree': 0.85,\n            'colsample_bynode': 0.85,\n            'min_child_samples': 10,           # 葉に含まれる最小データ数\n            'seed': seed                       # シード値\n        } \n \n        lgb_results = {}                       \n        model = lgb.train( \n            params,                            # ハイパーパラメータ\n            lgb_train,                         # 訓練データ\n            valid_sets=[lgb_train, lgb_valid], # 検証データ\n            valid_names=['Train', 'Valid'],    # データセット名前\n            num_boost_round=4000,              # 計算回数\n            early_stopping_rounds=-1,         # 計算打ち切り設定\n            evals_result=lgb_results,          # 学習の履歴\n            verbose_eval=100,                  # 学習過程の表示サイクル\n#             feval=feval_RMSPE,             # custom_metric\n#             fobj=rmspe_loss_lgb,                   # custom_metric\n        )  \n\n        ################################\n        # 結果描画\n        ################################\n        fig = plt.figure(figsize=(10, 8))\n\n        # loss\n        plt.subplot(1,2,1)\n        loss_train = lgb_results['Train']['rmse']\n        loss_test = lgb_results['Valid']['rmse']   \n        plt.xlabel('Iteration')\n        plt.ylabel('logloss')\n        plt.plot(loss_train, label='train loss')\n        plt.plot(loss_test, label='valid loss')\n        plt.legend()\n\n        # feature importance\n        plt.subplot(1,2,2)\n        importance = pd.DataFrame({'feature':feat_cols, 'importance':model.feature_importance()})\n        sns.barplot(x = 'importance', y = 'feature', data = importance.sort_values('importance', ascending=False))\n\n        plt.tight_layout()\n        plt.show()\n\n        ################################\n        # 評価\n        ################################\n        # 推論\n        pred =  model.predict(X_valid, num_iteration=model.best_iteration)\n        # 評価\n        score = evaluator(valid, pred)\n\n        scores.append(score)\n        models.append(model)\n\n    print(\"CV_SCORES:\", scores)\n    print(\"CV_SCORE:\", np.mean(scores))\n    \n    return models","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:43:47.462076Z","iopub.execute_input":"2022-06-30T13:43:47.462319Z","iopub.status.idle":"2022-06-30T13:43:49.550893Z","shell.execute_reply.started":"2022-06-30T13:43:47.462288Z","shell.execute_reply":"2022-06-30T13:43:49.550166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nimport pickle\n# import optuna.integration.lightgbm as lgb\n\ndef rmspe_loss_lgb(y_pred, train_dataset):\n\n    \"\"\"\n    Calculate gradient and hessian of the loss function root mean squared percentage error\n\n    Parameters\n    ----------\n    y_pred [array-like of shape (n_samples)]: Predictions\n    train_dataset (lightgbm.basic.Dataset): Training dataset\n\n    Returns\n    -------\n    gradient (float): First order derivative of the loss function root mean squared percentage error\n    hessian (float): Second order derivative of the loss function root mean squared percentage error\n    \"\"\"\n\n    y_true = train_dataset.get_label()\n    gradient = 2.0 / (y_true+1) * ((y_pred+1) * 1.0 / (y_true+1) - 1)\n    hessian = 2.0 / ((y_true+1) ** 2)\n\n    return gradient, hessian\n    \n    \ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / (y_true)))))\n\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\n# 学習を実行する関数\ndef trainer_quantile(feature_df, feat_cols, label_col, fold_params, seed=2022):\n    quantiles = [i / 16 for i in range(1, 16)]\n    scores = []\n    models = {q:[] for q in quantiles}\n    params = []\n    for q in quantiles:\n        print(f'====================train quantile: {q}========================')\n        for fold_id, param in enumerate(fold_params):\n            ################################\n            # データ準備\n            ################################\n            train = feature_df[(param[0] <= feature_df['Date']) & (feature_df['Date'] < param[1])]\n            valid = feature_df[(param[1] <= feature_df['Date']) & (feature_df['Date'] < param[2])]\n\n            X_train = train[feat_cols]\n            y_train = train[label_col]\n            X_valid = valid[feat_cols]\n            y_valid = valid[label_col]\n\n            lgb_train = lgb.Dataset(X_train, y_train)\n            lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n            ################################\n            # 学習\n            ################################\n            params = {\n                'device':'gpu',\n                'task': 'train',                   # 学習\n                'boosting_type': 'gbdt',           # GBDT\n                'objective': 'quantile',         # 回帰\n                'alpha': q,\n                'metric': 'rmse',                  # 損失（誤差）\n                'learning_rate': 0.01,             # 学習率\n                'lambda_l1': 0.5,                  # L1正則化項の係数\n                'lambda_l2': 0.5,                  # L2正則化項の係数\n                'num_leaves': 32,                  # 最大葉枚数\n                'max_depth':7,\n    #             'feature_fraction': 0.5,           # ランダムに抽出される列の割合\n                'bagging_fraction': 0.65,\n                'bagging_freq': 1, \n                'colsample_bytree': 0.85,\n                'colsample_bynode': 0.85,\n                'min_child_samples': 10,           # 葉に含まれる最小データ数\n                'seed': seed                       # シード値\n            } \n\n            lgb_results = {}                       \n            model = lgb.train( \n                params,                            # ハイパーパラメータ\n                lgb_train,                         # 訓練データ\n                valid_sets=[lgb_train, lgb_valid], # 検証データ\n                valid_names=['Train', 'Valid'],    # データセット名前\n                num_boost_round=4000,              # 計算回数\n                early_stopping_rounds=-1,         # 計算打ち切り設定\n                evals_result=lgb_results,          # 学習の履歴\n                verbose_eval=100,                  # 学習過程の表示サイクル\n    #             feval=feval_RMSPE,             # custom_metric\n    #             fobj=rmspe_loss_lgb,                   # custom_metric\n            )  \n\n            ################################\n            # 結果描画\n            ################################\n            fig = plt.figure(figsize=(10, 8))\n\n            # loss\n            plt.subplot(1,2,1)\n            loss_train = lgb_results['Train']['rmse']\n            loss_test = lgb_results['Valid']['rmse']   \n            plt.xlabel('Iteration')\n            plt.ylabel('logloss')\n            plt.plot(loss_train, label='train loss')\n            plt.plot(loss_test, label='valid loss')\n            plt.legend()\n\n            # feature importance\n            plt.subplot(1,2,2)\n            importance = pd.DataFrame({'feature':feat_cols, 'importance':model.feature_importance()})\n            sns.barplot(x = 'importance', y = 'feature', data = importance.sort_values('importance', ascending=False))\n\n            plt.tight_layout()\n            plt.show()\n            \n            models[q].append(model)\n            model_name = f'lgb_quantile_{q}_fold_{fold_id}.bin'\n            pickle.dump(model, open(model_name, 'wb'))\n#         ################################\n#         # 評価\n#         ################################\n#         # 推論\n#         pred =  model.predict(X_valid, num_iteration=model.best_iteration)\n#         # 評価\n#         score = evaluator(valid, pred)\n\n#         scores.append(score)\n#         models.append(model)\n\n#     print(\"CV_SCORES:\", scores)\n#     print(\"CV_SCORE:\", np.mean(scores))\n    \n    return models","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:43:49.552459Z","iopub.execute_input":"2022-06-30T13:43:49.552707Z","iopub.status.idle":"2022-06-30T13:43:49.573595Z","shell.execute_reply.started":"2022-06-30T13:43:49.552672Z","shell.execute_reply":"2022-06-30T13:43:49.572961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exec","metadata":{}},{"cell_type":"code","source":"# 2020-12-23よりも前のデータは証券コードが2000個すべて揃っていないため、これ以降のデータのみを使う。\n# (学習用データの開始日、学習用データの終了日＝検証用データの開始日、検証用データの終了日)\nfold_params = [\n    ('2018-04-01', '2020-05-01', '2020-06-01'),\n    ('2019-04-01', '2021-05-01', '2021-06-01'),\n    ('2020-04-01', '2022-05-01', '2022-06-01'),\n]\nmodels = trainer_quantile(feature_df, feat_cols, label_col, fold_params)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:43:49.574665Z","iopub.execute_input":"2022-06-30T13:43:49.575456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 推論・評価","metadata":{}},{"cell_type":"markdown","source":"生成したモデルを使って試験用データの推論を行いスコアを算出します。","metadata":{}},{"cell_type":"markdown","source":"## Function","metadata":{}},{"cell_type":"code","source":"def predictor(feature_df, feat_cols, models, is_train=True):\n    X = feature_df[feat_cols]\n    \n    # 推論\n    preds = list(map(lambda model: model.predict(X, num_iteration=model.best_iteration), models))\n    \n    # スコアは学習時のみ計算\n    if is_train:\n        scores = list(map(lambda pred: evaluator(feature_df, pred), preds))\n        print(\"SCORES:\", scores)\n\n    # 推論結果をバギング\n    pred = np.array(preds).mean(axis=0)\n\n    # スコアは学習時のみ計算\n    if is_train:\n        score = evaluator(feature_df, pred)\n        print(\"SCORE:\", score)\n    \n    return pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictor_quantile(feature_df, feat_cols, models, is_train=True):\n    X = feature_df[feat_cols]\n    q_preds = []\n    for q in models.keys():\n        # 推論\n        preds = list(map(lambda model: model.predict(X, num_iteration=model.best_iteration), models[q]))\n        # 推論結果をバギング\n        w = np.array([0.2, 0.2, 0.6])\n        pred = np.average(np.array(preds), axis=0, weights=w)\n\n        q_preds.append(pred)\n        \n    q_preds = np.array(q_preds)\n\n    means = q_preds.mean(axis=0)\n\n    var = q_preds.var(axis=0)\n    weight_preds = means / var\n\n#     # スコアは学習時のみ計算\n#     if is_train:\n#         scores = list(map(lambda pred: evaluator(feature_df, pred), preds))\n#         print(\"SCORES:\", scores)\n\n#     # 推論結果をバギング\n#     pred = np.array(preds).mean(axis=0)\n\n    # スコアは学習時のみ計算\n    if is_train:\n        score = evaluator(feature_df, means)\n        print(\"SCORE:\", score)\n    \n    return means","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exec","metadata":{}},{"cell_type":"code","source":"# 試験用データは学習用にも検証用にも使用していないものを使う\ntest_df = feature_df[('2022-02-01' <= feature_df['Date'])].copy()\n# pred = predictor(test_df, feat_cols, models)\npred = predictor_quantile(test_df, feat_cols, models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 提出","metadata":{}},{"cell_type":"markdown","source":"時系列APIを使って推論結果を登録します。  \n特徴量として、移動平均等の過去データを参照する値を採用しているため、時系列APIから得られたデータをため込む仕組みを実装する必要があります。  \nこの仕組みに関しては、[Start-to-finish demo based on s-meitoma + tweaks](https://www.kaggle.com/code/lowellniles/start-to-finish-demo-based-on-s-meitoma-tweaks)を参考にさせていただきました。  \n以下のコードでは`past_df`というデータフレームに履歴データをため込む実装になっています。","metadata":{}},{"cell_type":"code","source":"# 時系列APIのロード\nimport jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# supplemental filesを履歴データの初期状態としてセットアップ\npast_df = base_df[base_df['Date'] >= '2020-12-23'].copy()\npast_df = past_df.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 日次で推論・登録\nfor i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(iter_test):\n    print(f'iteration {i}')\n    current_date = prices[\"Date\"].iloc[0]\n\n    if i == 0:\n        # リークを防止するため、時系列APIから受け取ったデータより未来のデータを削除\n        past_df = past_df[past_df[\"Date\"] < current_date]\n\n#     # リソース確保のため古い履歴を削除\n#     threshold = (pd.Timestamp(current_date) - pd.offsets.BDay(80)).strftime(\"%Y-%m-%d\")\n#     past_df = past_df[past_df[\"Date\"] >= threshold]\n    \n    # 時系列APIから受け取ったデータを履歴データに統合\n    base_df = collector(prices, options, financials, trades, secondary_prices, stock_list)\n    past_df = pd.concat([past_df, base_df]).reset_index(drop=True)\n\n    # 特徴量エンジニアリング\n    feature_df, feat_cols, label_col = preprocessor(past_df, False)\n\n    # 予測対象レコードだけを抽出\n    feature_df = feature_df[feature_df['Date'] == current_date]\n\n    # 推論\n    feature_df[\"pred\"] = predictor_quantile(feature_df, feat_cols, models, False)\n\n    # 推論結果からRANKを導出し、提出データに反映\n    feature_df = add_rank(feature_df)\n    feature_map = feature_df.set_index('SecuritiesCode')['Rank'].to_dict()\n    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(feature_map)\n    \n\n    # 結果を登録\n    env.predict(sample_prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}