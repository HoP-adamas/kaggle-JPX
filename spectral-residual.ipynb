{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.kaggle.com/code/ikeppyo/jpx-lightgbm-demo\nをfork。","metadata":{}},{"cell_type":"markdown","source":"データの詳細は https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction/data を参照  \n時系列APIの使い方は https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction/overview/evaluation を参照  \nオプションコード体系の日本語版は https://www.jpx.co.jp/sicc/securities-code/nlsgeu0000032d48-att/(HP)sakimono20220208.pdf を参照  \n証券コード関係は https://www.jpx.co.jp/sicc/securities-code/01.html を参照","metadata":{}},{"cell_type":"markdown","source":"# 準備\n* 使用するライブラリをインポートします","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nfrom pathlib import Path\nfrom decimal import ROUND_HALF_UP, Decimal\n\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\nimport warnings\nwarnings.simplefilter('ignore')\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:00:22.336649Z","iopub.execute_input":"2022-06-26T07:00:22.338304Z","iopub.status.idle":"2022-06-26T07:00:23.868946Z","shell.execute_reply.started":"2022-06-26T07:00:22.338221Z","shell.execute_reply":"2022-06-26T07:00:23.867970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if (c_min > np.iinfo(np.int8).min) & (c_max < np.iinfo(np.int8).max):\n                    df[col] = df[col].astype(np.int8)\n                elif (c_min > np.iinfo(np.int16).min) & (c_max < np.iinfo(np.int16).max):\n                    df[col] = df[col].astype(np.int16)\n                elif (c_min > np.iinfo(np.int32).min) & (c_max < np.iinfo(np.int32).max):\n                    df[col] = df[col].astype(np.int32)\n                elif (c_min > np.iinfo(np.int64).min) & (c_max < np.iinfo(np.int64).max):\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if (c_min > np.finfo(np.float16).min) & (c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > np.finfo(np.float32).min) & (c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:00:23.870567Z","iopub.execute_input":"2022-06-26T07:00:23.870789Z","iopub.status.idle":"2022-06-26T07:00:23.891378Z","shell.execute_reply.started":"2022-06-26T07:00:23.870761Z","shell.execute_reply":"2022-06-26T07:00:23.890430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# データ読み込み\n\n以下のファイル群を読み込みます。\n* stock_list.csv\n* train_files 配下のファイル（Dateの範囲は2017-01-04 ～ 2021-12-03）\n* supplemental_files 配下のファイル（Dateの範囲は2021-12-06 ～ 2022-02-28 ※2022/04/05現在）","metadata":{}},{"cell_type":"markdown","source":"## Function","metadata":{}},{"cell_type":"code","source":"def read_files(dir_name: str = 'train_files'):\n    base_path = Path(f'../input/jpx-tokyo-stock-exchange-prediction/{dir_name}')\n    prices = pd.read_csv(base_path / 'stock_prices.csv')\n    options = pd.read_csv(base_path / 'options.csv')\n    financials = pd.read_csv(base_path / 'financials.csv')\n    trades = pd.read_csv(base_path / 'trades.csv')\n    secondary_prices = pd.read_csv(base_path / 'secondary_stock_prices.csv')\n    \n#     prices = reduce_mem_usage(prices)\n#     options = reduce_mem_usage(options)\n#     financials = reduce_mem_usage(financials)\n#     trades = reduce_mem_usage(trades)\n#     secondary_prices = reduce_mem_usage(secondary_prices)\n    \n    return prices, options, financials, trades, secondary_prices","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:00:23.892718Z","iopub.execute_input":"2022-06-26T07:00:23.893064Z","iopub.status.idle":"2022-06-26T07:00:23.912491Z","shell.execute_reply.started":"2022-06-26T07:00:23.893028Z","shell.execute_reply":"2022-06-26T07:00:23.911165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exec","metadata":{}},{"cell_type":"code","source":"%%time\n\nstock_list = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv')\ntrain_files = read_files('train_files')\nsupplemental_files = read_files('supplemental_files')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:00:23.914793Z","iopub.execute_input":"2022-06-26T07:00:23.915445Z","iopub.status.idle":"2022-06-26T07:01:07.271015Z","shell.execute_reply.started":"2022-06-26T07:00:23.915406Z","shell.execute_reply":"2022-06-26T07:01:07.269961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# データ統合\n\n* 読み込んだデータを結合して、一つのファイルに纏めます。","metadata":{}},{"cell_type":"markdown","source":"## Function","metadata":{}},{"cell_type":"markdown","source":"`merge_data`関数では各ファイルを水平方向に結合します。現時点では`stock_prices`、`stock_list`しか使っていません。  \nコメントアウトを解除すれば`trades`、`financials`とも結合は可能ですが、  \nこれらのデータは、有効なレコードが発生するタイミングが日次ではないため、学習データとして意味のあるものとするためには「直近に発生した有効なレコードの値を引き継ぐ」などの対処が必要となります。  \n`options`に関しては、[OptionsCodeの附番規則](https://www.jpx.co.jp/sicc/securities-code/nlsgeu0000032d48-att/(HP)sakimono20220208.pdf)を見ていけば適切な使い方が見えそうです。","metadata":{}},{"cell_type":"code","source":"def merge_data(prices, options, financials, trades, secondary_prices, stock_list):\n    # stock_prices がベース\n    base_df = prices.copy()\n    \n    # stock_listと結合\n    _stock_list = stock_list.copy()\n    _stock_list.rename(columns={'Close': 'Close_x'}, inplace=True)\n    base_df = base_df.merge(_stock_list, on='SecuritiesCode', how=\"left\")\n\n    # tradesと結合\n    # stock_listのNewMarketSegmentと紐づくよう、tradesのSection項目を編集する\n    # _trades = trades.copy()\n    # _trades['NewMarketSegment'] = _trades['Section'].str.split(' \\(', expand=True)[0]\n    # base_df = base_df.merge(_trades, on=['Date', 'NewMarketSegment'], how=\"left\")\n\n#     # financialsと結合\n#     _financials = financials.copy()\n#     _financials.rename(columns={'Date': 'Date_x', 'SecuritiesCode': 'SecuritiesCode_x'}, inplace=True)\n#     use_cols = ['DateCode',\n#                'NetSales',\n#                'OperatingProfit',\n#                'OrdinaryProfit',\n#                'Profit', \n#                'EarningsPerShare', \n#                'TotalAssets', \n#                'Equity', \n#                'EquityToAssetRatio', \n#                'NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock']\n#     # メモリがカツカツなのでカラムを使うものだけに絞る\n#     _financials = _financials[use_cols]\n#     _financials.replace(['-', '－'], np.nan, inplace=True)\n#     for col in use_cols[1:]:\n#         _financials[col] = _financials[col].astype(float)\n#     prev_column = set(_financials.columns)\n    \n#     # 同じDateCodeで複数行あるのでまとめあげる\n#     _financials = _financials.groupby('DateCode').max().reset_index()\n#     assert prev_column == set(_financials.columns.to_list())\n#     _financials.drop_duplicates(subset='DateCode', inplace=True)\n#     assert prev_column == set(_financials.columns.to_list())\n#     base_df = base_df.merge(_financials, left_on='RowId', right_on='DateCode', how=\"left\")\n    \n    return base_df","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:07.273400Z","iopub.execute_input":"2022-06-26T07:01:07.273968Z","iopub.status.idle":"2022-06-26T07:01:07.285434Z","shell.execute_reply.started":"2022-06-26T07:01:07.273890Z","shell.execute_reply":"2022-06-26T07:01:07.284233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`adjust_price`関数は[Train Demo](https://www.kaggle.com/code/smeitoma/train-demo)で紹介されている関数をほぼそのまま使わせて頂いています。（Dateのインデックス化だけコメントアウトしています）  \n項目追加が発生するため、統合の範囲を超えていますが、関数内でソートやインデックスの生成といった操作が行われるため、この段階で実行しています。  \n  \n関数では**AdjustedClose**という項目が生成されます。  \n株式は、分割や併合によって株価が大きく変動することがありますが、**Close**の代わりに**AdjustedClose**を使うことで、この影響を減少させることができるとのことです。","metadata":{}},{"cell_type":"code","source":"def adjust_price(price):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n    # transform Date column into datetime\n    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n\n    def generate_adjusted_close(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n\n    # price.set_index(\"Date\", inplace=True)\n    return price","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:07.287933Z","iopub.execute_input":"2022-06-26T07:01:07.288488Z","iopub.status.idle":"2022-06-26T07:01:07.311511Z","shell.execute_reply.started":"2022-06-26T07:01:07.288444Z","shell.execute_reply":"2022-06-26T07:01:07.310496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collector(prices, options, financials, trades, secondary_prices, stock_list):\n    # 読み込んだデータを統合して一つのファイルに纏める\n    base_df = merge_data(prices, options, financials, trades, secondary_prices, stock_list)\n    \n    # AdjustedClose項目の生成\n    base_df = adjust_price(base_df)\n    \n    return base_df","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:07.314105Z","iopub.execute_input":"2022-06-26T07:01:07.314646Z","iopub.status.idle":"2022-06-26T07:01:07.332463Z","shell.execute_reply.started":"2022-06-26T07:01:07.314518Z","shell.execute_reply":"2022-06-26T07:01:07.331458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exec","metadata":{}},{"cell_type":"markdown","source":"supplemental filesを使わない場合は4行目以降をコメントアウトします。","metadata":{}},{"cell_type":"code","source":"%%time\n\nbase_df = collector(*train_files, stock_list)\nsupplemental_df = collector(*supplemental_files, stock_list)\nbase_df = pd.concat([base_df, supplemental_df]).reset_index(drop=True)\n\n# 東証が障害で止まった日は除外\nbase_df = base_df[base_df['Date'] != '2020-10-01']\n\n# base_df['Target'] = base_df['Target'] * 100","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:07.334423Z","iopub.execute_input":"2022-06-26T07:01:07.334963Z","iopub.status.idle":"2022-06-26T07:01:51.198046Z","shell.execute_reply.started":"2022-06-26T07:01:07.334923Z","shell.execute_reply":"2022-06-26T07:01:51.196934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:51.199533Z","iopub.execute_input":"2022-06-26T07:01:51.200499Z","iopub.status.idle":"2022-06-26T07:01:51.419901Z","shell.execute_reply.started":"2022-06-26T07:01:51.200453Z","shell.execute_reply":"2022-06-26T07:01:51.418838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_df","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:51.423338Z","iopub.execute_input":"2022-06-26T07:01:51.423578Z","iopub.status.idle":"2022-06-26T07:01:51.492895Z","shell.execute_reply.started":"2022-06-26T07:01:51.423549Z","shell.execute_reply":"2022-06-26T07:01:51.492080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_rank_with_new_col_name(df, col_name=\"pred\", new_col_name='Rank', ascend=False):\n    df[new_col_name] = df.groupby(\"Date\")[col_name].rank(ascending=ascend, method=\"first\") - 1 \n    df[new_col_name] = df[new_col_name].astype(\"int\")\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:51.494350Z","iopub.execute_input":"2022-06-26T07:01:51.494571Z","iopub.status.idle":"2022-06-26T07:01:51.500098Z","shell.execute_reply.started":"2022-06-26T07:01:51.494538Z","shell.execute_reply":"2022-06-26T07:01:51.499322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# https://arxiv.org/abs/2012.07245 の実装","metadata":{}},{"cell_type":"markdown","source":"# spectral residualの抽出","metadata":{}},{"cell_type":"code","source":"def generate_return_df(base_df):\n    \"\"\"\n    欠損値は0で埋める\n    \"\"\"\n    tmp = base_df[[\"RowId\", \"Date\", \"SecuritiesCode\", \"AdjustedClose\"]]\n    tmp = tmp.sort_values(['Date', 'SecuritiesCode'])\n    tmp[\"return\"] = tmp.groupby(\"SecuritiesCode\")[\"AdjustedClose\"].pct_change()\n    tmp['return'].fillna(0, inplace=True)\n    \n    ret = tmp.pivot(index='Date', columns='SecuritiesCode', values='return')\n    ret.fillna(0, inplace=True)\n    return ret\n\ndef generate_log_return_df(base_df):\n    \"\"\"\n    欠損値は0で埋める\n    \"\"\"\n    tmp = base_df[[\"RowId\", \"Date\", \"SecuritiesCode\", \"AdjustedClose\"]]\n    tmp = tmp.sort_values(['Date', 'SecuritiesCode'])\n    tmp[\"log_return\"] = tmp.groupby(\"SecuritiesCode\")[\"AdjustedClose\"].apply(np.log).diff()\n    tmp['log_return'].fillna(0, inplace=True)\n    \n    ret = tmp.pivot(index='Date', columns='SecuritiesCode', values='log_return')\n    ret.fillna(0, inplace=True)\n    return ret\n\ndef generate_log_return_df_per_code(base_df):\n    tmp = base_df[[\"RowId\", \"Date\", \"SecuritiesCode\", \"AdjustedClose\", \"33SectorCode\"]]\n    tmp = tmp.sort_values(['Date', 'SecuritiesCode'])\n    tmp[\"log_return\"] = tmp.groupby(\"SecuritiesCode\")[\"AdjustedClose\"].apply(np.log).diff()\n    tmp['log_return'].fillna(0, inplace=True)\n    \n    ret_dict = dict()\n    for sectorCode in tmp['33SectorCode'].unique():\n        ret = tmp[tmp['33SectorCode'] == sectorCode].pivot(index='Date', columns='SecuritiesCode', values='log_return')\n        ret.fillna(0, inplace=True)\n        for seccode in ret.columns:\n            if not np.all(ret[seccode][-20:] != 0):\n                print('drop', seccode)\n                ret.drop(seccode, axis=1, inplace=True)\n        ret_dict[sectorCode] = ret\n\n    return ret_dict","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:51.501426Z","iopub.execute_input":"2022-06-26T07:01:51.501613Z","iopub.status.idle":"2022-06-26T07:01:51.517083Z","shell.execute_reply.started":"2022-06-26T07:01:51.501587Z","shell.execute_reply":"2022-06-26T07:01:51.516573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"return_df = generate_return_df(base_df)\nreturn_df.head()\nlog_return_df = generate_log_return_df(base_df)\nlog_return_per_code_df = generate_log_return_df_per_code(base_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:25:20.628816Z","iopub.execute_input":"2022-06-26T07:25:20.629126Z","iopub.status.idle":"2022-06-26T07:25:37.878305Z","shell.execute_reply.started":"2022-06-26T07:25:20.629094Z","shell.execute_reply":"2022-06-26T07:25:37.877487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5月27日の段階で市場から撤退している銘柄がある。\n# おそらく、1413, 4699, 8806\n# mask = return_df.loc[\"2022-05-27\"].isnull()\n# print(return_df.columns[mask])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:57.123092Z","iopub.execute_input":"2022-06-26T07:01:57.123318Z","iopub.status.idle":"2022-06-26T07:01:57.127582Z","shell.execute_reply.started":"2022-06-26T07:01:57.123291Z","shell.execute_reply":"2022-06-26T07:01:57.126264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# generate_residual_spectral\n$$\nf(\\mathbf{[\\mathbf{r_{t-H+1}}, \\cdots, \\mathbf{r_{t}}}]):\\mathbb{R}^{N\\times H} \\rightarrow \\mathbb{R}^{N\\times H}\n$$","metadata":{}},{"cell_type":"code","source":"# residual_spectralを生成する\n# N×H行列\ndef generate_residual_spectral(return_df, current_date):\n    horizon = 64\n    num_remove_factors = 3\n    date_idx_map = {}\n    for i, date in enumerate(return_df.index):\n        date_idx_map[date] = i\n    idx = date_idx_map[current_date]\n    assert idx > horizon\n    # 直近256日分のデータをとってくる。\n    left_idx = idx-horizon+1\n    X = return_df[return_df.index[left_idx]:return_df.index[idx]]\n    \n    # 列方向を時間軸にする\n    X = X.transpose()\n    # 中心化\n    X_tilde = X.apply(lambda x: x - x.mean(), axis=1)\n    v, s, uh = np.linalg.svd(X_tilde.fillna(0).to_numpy(), full_matrices=False, compute_uv=True)\n\n    for i in range(len(s)):\n        s[i] = 0 if i < num_remove_factors else 1\n    A = v @ np.diag(s)  @ v.T\n    ret = pd.DataFrame(index=X_tilde.index, columns=X_tilde.columns)\n    for date in return_df.index[idx-horizon+1:idx+1]:\n        ret[date] = A @ X[date]\n        \n    for code in ret.index:\n        ret.loc[code] = ret.loc[code] / np.linalg.norm(ret.loc[code].to_numpy())\n    return ret","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:25:37.879914Z","iopub.execute_input":"2022-06-26T07:25:37.880162Z","iopub.status.idle":"2022-06-26T07:25:37.896018Z","shell.execute_reply.started":"2022-06-26T07:25:37.880115Z","shell.execute_reply":"2022-06-26T07:25:37.893513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model\n\nmodelを写像$\\psi(\\mathbf{x})$とすると、\n$$\n\\psi(\\mathbf{x}): \\mathbb{R}^H \\rightarrow \\mathbb{R}^{Q-1} \\\\\n\\psi(\\mathbf{x}) = \\psi_2\\left(\\frac{1}{L}\\sum_{i=1}^{L}{\\psi_1(Resample(\\mathbf{x}, \\tau_i}))\\right)\\\\\n\\psi_1: \\mathbb{R}^{H'} \\rightarrow \\mathbb{R}^{K}, where\\ K=256\\\\\n\\psi_2: \\mathbb{R}^{K} \\rightarrow \\mathbb{R}^{Q-1}\n$$\n\n## distributional prediction\n銘柄ごとの直近H日間の残差リターン$\\tilde{\\epsilon}_{i, t-H}, \\tilde{\\epsilon}_{i, t-H+1}, \\cdots, \\tilde{\\epsilon}_{i, t-1}$が与えられたときに、1つ後の残差リターンの分布を予測する。  \nすなわち、考えるのは以下の条件付き分布である。\n\n$$\np(\\tilde{\\epsilon}_{i, t}|\\tilde{\\epsilon}_{i, t-H}, \\tilde{\\epsilon}_{i, t-H+1}, \\cdots, \\tilde{\\epsilon}_{i, t-1})\n$$\n\nこれをノンパラメトリックに推定するために、32分位点回帰を行って、各分位点の予測値を出すことで分布の平均と分散を求める。\n\n今回の問題設定の場合は、2つ後の残差リターンを予測することになる。\n\n$$\np(\\tilde{\\epsilon}_{i, t+2}|\\tilde{\\epsilon}_{i, t-H+1}, \\tilde{\\epsilon}_{i, t-H+1}, \\cdots, \\tilde{\\epsilon}_{i, t})\n$$\n\n## Network Architectures\n\n### Volatility invariance\nボラティリティクラスタリングは、ボラティリティについてのスケール不変性と考えられる。これをモデルに組み込むために、モデルは正の同次な写像であるとする。\n\n### Fractal structure\n株価はフラクタル性を持っていることで知られるため、異なる長さの区間のデータを取ってきて、同じ幅になるようにリサンプリングし、同じネットワークに流す。この際、時間軸のスケールファクターでリスケーリングを行う。このファクターはHurst指数が株価の場合はおよそ0.5であることが観測からわかっているため、$\\tau^{0.5}$で割る。\n\n### resample\nフラクタルネットワークに流すためにリサンプリングを行う関数。これは一つの銘柄のH期間のデータを入力とし、固定長H'のベクトルを返す。\n$$\nResample(x, \\tau): \\mathbb{R}^{H} \\rightarrow \\mathbb{R}^{H'}\\\\\n\\tau: scale\\ parameter\n$$\n\nこの関数の処理は4段階からなっている。\n\n1. 累積和\n1. Resample\n1. 一階差分\n1. Rescale\n\nまた、このResample関数は正の同次性を持っている。\n","metadata":{}},{"cell_type":"code","source":"import math\ndef resample(x, tau, sample_size=64):\n    ret = []\n    for i in range(len(x)):\n        row = x[i]\n        z = []\n        s = 0\n        for xx in row:\n            s += xx\n            z.append(s)\n        z_tau = z[-math.ceil(len(z)*tau):]\n\n        resampled_z = []\n        pos = 0\n        while pos < len(z_tau):\n            resampled_z.append(z_tau[int(pos)])\n            pos += len(z_tau) / sample_size\n\n        diff = [resampled_z[0]] + [resampled_z[i+1] - resampled_z[i] for i in range(len(resampled_z)-1)]\n        diff = np.array(diff)\n        diff *= tau**(-1/2)\n\n        ret.append(diff)\n        assert len(diff) == sample_size\n    ret = np.array(ret)\n    assert np.isnan(ret).any() == False\n\n    return ret\n\n# model\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass FractalNetwork(nn.Module):\n    \n    def __init__(self, dim_input=64, n_hidden=256, Q=32, device=None):\n        super().__init__()\n        if device is None:\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = device\n        self.dim_input = dim_input\n        self.n_hidden = n_hidden\n        self.Q = Q\n        self.psi1 = Psi1(dim_input=dim_input, K=n_hidden)\n        self.psi2 = Psi2(dim_input=n_hidden, Q=Q)\n        \n    def forward(self, inputs):\n        psi1_result = torch.zeros((inputs.shape[0], 256), device=torch.device(self.device))\n#         psi1_result.to(self.device)\n#         print('psi1_result device:', psi1_result.device)\n        L = 21\n        for tau in [4**(-j/(L-1)) for j in range(L)]:\n            resampled_inputs = resample(inputs, tau, self.dim_input)\n            resampled_inputs = torch.from_numpy(resampled_inputs.astype(np.float32))\n            resampled_inputs = resampled_inputs.to(self.device)\n#             print('resampled_inputs device:', resampled_inputs.device)\n#             tmp = self.psi1(resampled_inputs)\n#             print('debug:psi1_result', tmp)\n#             print(tmp.shape)\n#             print('psi1.device: ',self.psi1.device)\n            psi1_result += self.psi1(resampled_inputs)\n        psi1_result /= L\n        \n        return self.psi2(psi1_result)\n    \nclass Psi1(nn.Module):\n    \n    def __init__(self, device=None, dim_input=64, K=256):\n        super().__init__()\n        if device is None:\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = device\n            \n        self.layers = nn.Sequential(\n            torch.nn.Linear(dim_input, 256, bias=False),\n            torch.nn.BatchNorm1d(256),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(256, 256, bias=False),\n            torch.nn.BatchNorm1d(256),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(256, 256, bias=False),\n            torch.nn.BatchNorm1d(256),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(256, K, bias=False)\n            )\n        \n    def forward(self, inputs):\n        return self.layers(inputs)\n    \nclass Psi2(nn.Module):\n      \n    def __init__(self, device=None, dim_input=256, Q=32):\n        super().__init__()\n        if device is None:\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = device\n\n        self.layers = nn.Sequential(\n            torch.nn.Linear(dim_input, 128, bias=False), #1\n            torch.nn.BatchNorm1d(128),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(128, 128, bias=False), #2\n            torch.nn.BatchNorm1d(128),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(128, 128, bias=False), #3\n            torch.nn.BatchNorm1d(128),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(128, 128, bias=False), #4\n            torch.nn.BatchNorm1d(128),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(128, 128, bias=False), #5\n            torch.nn.BatchNorm1d(128),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(128, 128, bias=False), #6\n            torch.nn.BatchNorm1d(128),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(128, 128, bias=False), #7\n            torch.nn.BatchNorm1d(128),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(128, 128, bias=False), #8\n            torch.nn.BatchNorm1d(128),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.5),\n            torch.nn.Linear(128, Q-1, bias=False)\n        )\n\n\n    def forward(self, inputs):\n        return self.layers(inputs)\n            \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:25:37.897975Z","iopub.execute_input":"2022-06-26T07:25:37.898316Z","iopub.status.idle":"2022-06-26T07:25:37.948402Z","shell.execute_reply.started":"2022-06-26T07:25:37.898275Z","shell.execute_reply":"2022-06-26T07:25:37.947129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function\nclass QuantileLoss(nn.Module):\n    def __init__(self, Q=32):\n        super(QuantileLoss, self).__init__()\n        self.Q = Q\n        \n    def forward(self, outputs, targets, device):\n        batch_loss = torch.zeros((outputs.shape[0], 1), device=torch.device(device))\n        for i in range(outputs.shape[0]):\n            assert outputs.shape[1] == self.Q-1\n            output = outputs[i]\n            losses = torch.zeros(self.Q-1, device=torch.device(device))\n\n            for j in range(self.Q - 1):\n                alpha = (j+1) / self.Q\n                diff = targets[i] - output[j]\n                losses[j] = torch.max((alpha-1)*diff, alpha*diff)\n            batch_loss[i] = (torch.sum(losses))\n#         batch_loss = torch.cat(batch_loss, dim=1)\n        return torch.mean(batch_loss)\n            \n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:25:37.951165Z","iopub.execute_input":"2022-06-26T07:25:37.951465Z","iopub.status.idle":"2022-06-26T07:25:37.967981Z","shell.execute_reply.started":"2022-06-26T07:25:37.951424Z","shell.execute_reply":"2022-06-26T07:25:37.967179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nclass MyDataset(Dataset):\n    \n    def __init__(self, residual_spectral, targets):\n        super().__init__()\n        self.residual_spectral = residual_spectral\n        self.targets = targets\n        self.idx_map = dict()\n        self.inverse_idx_map = dict()\n        for i, code in enumerate(residual_spectral.index):\n            self.idx_map[code] = i\n            self.inverse_idx_map[i] = code\n    \n    def __getitem__(self, idx):\n        inputs = self.residual_spectral.loc[self.inverse_idx_map[idx]].values.astype(np.float32)\n        target = self.targets.loc[self.inverse_idx_map[idx]].astype(np.float32)\n        return inputs, target\n    \n    def __len__(self):\n        return len(self.residual_spectral)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:25:37.969647Z","iopub.execute_input":"2022-06-26T07:25:37.970104Z","iopub.status.idle":"2022-06-26T07:25:37.988452Z","shell.execute_reply.started":"2022-06-26T07:25:37.970052Z","shell.execute_reply":"2022-06-26T07:25:37.987415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:25:37.989933Z","iopub.execute_input":"2022-06-26T07:25:37.990414Z","iopub.status.idle":"2022-06-26T07:25:38.265976Z","shell.execute_reply.started":"2022-06-26T07:25:37.990374Z","shell.execute_reply":"2022-06-26T07:25:38.264357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# calculate the weights of portfolio from the prediction of quantiles of distribution\nmodelの出力値である分位点の予測値からポートフォリオのウェイトを計算する。\nsubmitの形式にする場合は、事前にdataframeを作っておき、銘柄のカラムを追加、Dateのカラムは予測日とする。\n\nその後、calc_weightsの返り値をpredカラムの値とし、その値からadd_rankで順位付けを行う。\n","metadata":{}},{"cell_type":"code","source":"def get_mean_and_variance(outputs):\n    means = torch.mean(outputs.detach(), dim=1, keepdim=True)\n    variances = torch.var(outputs.detach(), dim=1, keepdim=True)\n    return means, variances\n\ndef calc_weights(means, variances):\n    ret = means / variances\n    ret = ret.cpu()\n    return ret.numpy()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:58.973793Z","iopub.execute_input":"2022-06-26T07:01:58.974412Z","iopub.status.idle":"2022-06-26T07:01:58.982964Z","shell.execute_reply.started":"2022-06-26T07:01:58.974371Z","shell.execute_reply":"2022-06-26T07:01:58.982175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:58.984178Z","iopub.execute_input":"2022-06-26T07:01:58.984515Z","iopub.status.idle":"2022-06-26T07:01:59.000154Z","shell.execute_reply.started":"2022-06-26T07:01:58.984486Z","shell.execute_reply":"2022-06-26T07:01:58.999513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train\n今は2000銘柄のデータを同時に入力している。\n\nbatch_size=2000\n\nのイメージ\n\nもしかしたらもっと細かく分割したほうがいいかもしれない。その場合は、データを作成する際にうまくreshapeする。DataLoaderを使ってもいいかもしれない。\n\nまた、validationも学習した日の次の日のデータで行っているが、235日分のデータはtrainで使用したものと同じなので、もっと離したほうがいいかもしれない。\n\n## version4\n予測させようとするとすべてのデータに対して同じ出力になってしまった。  \nなので、batch_sizeを小さくしてみる。\n\n## version5\nbatch_size=10\n効果なし\n\n## version6\n学習が進むにつれ同じ値になっていくことがわかった。  \nまた、一日ずれた日付の予測も同じになる。  \n長期のhorizonでやっているためかもしれないので、horizonを短くしてやってみる。  \nこれを変更するにあたって、除く特異ベクトルの数、resampleの数なども変える必要がある。","metadata":{}},{"cell_type":"code","source":"import random\ndef torch_fix_seed(seed=42):\n    # Python random\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    # Pytorch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.use_deterministic_algorithms = True\n\n\ntorch_fix_seed()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:59.001580Z","iopub.execute_input":"2022-06-26T07:01:59.003099Z","iopub.status.idle":"2022-06-26T07:01:59.024714Z","shell.execute_reply.started":"2022-06-26T07:01:59.003045Z","shell.execute_reply":"2022-06-26T07:01:59.024056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SectorCodeごとの学習\nepochs = 4\nmodel = FractalNetwork(dim_input=32)\ndevice = model.device\ncriterion = QuantileLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nmodel.train()\nmodel.to(device)\nprint(model)\n\ntrain_end_date = pd.to_datetime('2022-02-01')\n\nfor epoch in range(epochs):\n    losses = []\n    valid_losses = []\n    for i, date in tqdm(list(enumerate(log_return_df.index[:-43]))[-360:-60:10]):\n        if i <= 256 or date < pd.to_datetime('2020-12-23'):\n            continue\n        if date > train_end_date:\n            break\n        print('train date:', date)\n        preds_df = pd.DataFrame()\n        for code, log_return_df in log_return_per_code_df.items():\n            target_date = log_return_df.index[i+2]\n            residual_spectral = generate_residual_spectral(log_return_df, target_date)\n\n\n            valid_target_dates = log_return_df.index[-50:-40]\n            valid_residual_spectrals = [generate_residual_spectral(log_return_df, valid_date) for valid_date in valid_target_dates]\n\n            train_dataset = MyDataset(residual_spectral, log_return_df.loc[target_date])\n    #         valid_dataset = MyDataset(valid_residual_spectral, return_df.loc[valid_target_date])\n\n            train_loader = DataLoader(train_dataset, batch_size=len(log_return_df.columns), shuffle=True)\n    #         valid_loader = DataLoader(valid_dataset, batch_size=len(return_df.columns))\n            batch_train_loss = 0\n            batch_valid_loss = 0\n            for j, (train_inputs, train_targets) in enumerate(train_loader):\n                model.train()\n                outputs = model(train_inputs)\n                outputs = outputs.to(device)\n                loss = criterion(outputs, train_targets, device)\n                loss = loss.to(device)\n                batch_train_loss += loss.item()\n\n                optimizer.zero_grad()\n                loss.backward()\n\n                optimizer.step()\n            losses.append(batch_train_loss/(j+1))\n\n            with torch.no_grad():\n                model.eval()\n                \n                for ind, valid_target_date in enumerate(valid_target_dates):\n                    valid_residual_spectral = valid_residual_spectrals[ind]\n                    valid_dataset = MyDataset(valid_residual_spectral, log_return_df.loc[valid_target_date])\n                    valid_loader = DataLoader(valid_dataset, batch_size=len(log_return_df.columns))\n                    for k, (valid_inputs, valid_targets) in enumerate(valid_loader):\n                        valid_outputs = model(valid_inputs)\n                        valid_loss = criterion(valid_outputs, valid_targets, device)\n                        batch_valid_loss += valid_loss.item()\n                    means, variance = get_mean_and_variance(valid_outputs)\n                    weights = calc_weights(means, variance)\n    #                 print(weights)\n                    pred_df = pd.DataFrame()\n                    pred_df['SecuritiesCode'] = log_return_df.columns\n                    pred_df['Date'] = valid_target_date\n                    pred_df = pred_df.merge(base_df[['Date', 'SecuritiesCode', 'Target']], on=['Date','SecuritiesCode'],how='left')\n#                     pred_df['Target'] = log_return_df.loc[valid_target_date].values\n                    pred_df['pred'] = weights.squeeze()\n                    pred_df.fillna(0, inplace=True)\n                    preds_df = pd.concat([preds_df, pred_df])\n                    preds_df = preds_df.reset_index(drop=True)\n        preds_df = add_rank_with_new_col_name(preds_df, col_name='pred', new_col_name='Rank')\n        preds_df = add_rank_with_new_col_name(preds_df, col_name='Target', new_col_name='TrueRank')\n        sns.scatterplot(data=preds_df, x='pred', y='Target', hue='Date', alpha=0.5)\n        fig = plt.figure(figsize=(10, 20))\n        plt.show()\n        print(preds_df)\n        score = calc_spread_return_sharpe(preds_df)\n        print('score:', score)\n\n        valid_losses.append(batch_valid_loss/(k+1))\n        print(f'{i+1}\\t loss: ', losses[-1], '\\t', 'valid_loss: ', valid_losses[-1])\n#         if i %10 == 9:\n#             print(f'{i+1}\\t loss: ', losses[-1], '\\t', 'valid_loss: ', valid_losses[-1])\n        \n            \n            \n    plt.figure()\n    plt.plot(range(1, len(losses)+1), losses, label='train_loss')\n    plt.plot(range(1, len(valid_losses)+1), valid_losses, label='valid_loss')\n    plt.xlabel('iteration')\n    plt.legend()\n    plt.show()\n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:26:00.589356Z","iopub.execute_input":"2022-06-26T07:26:00.589655Z","iopub.status.idle":"2022-06-26T16:04:34.785106Z","shell.execute_reply.started":"2022-06-26T07:26:00.589625Z","shell.execute_reply":"2022-06-26T16:04:34.783632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 1\nbatch_size = 5\nmodel = FractalNetwork(dim_input=32)\ndevice = model.device\ncriterion = QuantileLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nmodel.train()\nmodel.to(device)\nprint(model)\n\ntrain_end_date = pd.to_datetime('2022-02-01')\n\nfor epoch in range(epochs):\n    losses = []\n    valid_losses = []\n    for i, date in tqdm(list(enumerate(log_return_df.index[:-43]))[-360:-60:10]):\n        if i <= 256 or date < pd.to_datetime('2020-12-23'):\n            continue\n        if date > train_end_date:\n            break\n        print('train date:', date)\n        target_date = log_return_df.index[i+2]\n        residual_spectral = generate_residual_spectral(log_return_df, target_date)\n        \n        \n        valid_target_dates = log_return_df.index[-50:-40]\n        valid_residual_spectrals = [generate_residual_spectral(log_return_df, valid_date) for valid_date in valid_target_dates]\n        \n        train_dataset = MyDataset(residual_spectral, log_return_df.loc[target_date])\n#         valid_dataset = MyDataset(valid_residual_spectral, return_df.loc[valid_target_date])\n        \n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n#         valid_loader = DataLoader(valid_dataset, batch_size=len(return_df.columns))\n        batch_train_loss = 0\n        batch_valid_loss = 0\n        for j, (train_inputs, train_targets) in enumerate(train_loader):\n            model.train()\n            outputs = model(train_inputs)\n            outputs = outputs.to(device)\n            loss = criterion(outputs, train_targets, device)\n            loss = loss.to(device)\n            batch_train_loss += loss.item()\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n        losses.append(batch_train_loss/(j+1))\n        \n        with torch.no_grad():\n            model.eval()\n            preds_df = pd.DataFrame()\n            for ind, valid_target_date in enumerate(valid_target_dates):\n                valid_residual_spectral = valid_residual_spectrals[ind]\n                valid_dataset = MyDataset(valid_residual_spectral, log_return_df.loc[valid_target_date])\n                valid_loader = DataLoader(valid_dataset, batch_size=len(log_return_df.columns))\n                for k, (valid_inputs, valid_targets) in enumerate(valid_loader):\n                    valid_outputs = model(valid_inputs)\n                    valid_loss = criterion(valid_outputs, valid_targets, device)\n                    batch_valid_loss += valid_loss.item()\n                print('debug')\n                print(valid_outputs)\n                means, variance = get_mean_and_variance(valid_outputs)\n                weights = calc_weights(means, variance)\n                print(means, variance)\n#                 print(weights)\n                pred_df = pd.DataFrame()\n                pred_df['SecuritiesCode'] = log_return_df.columns\n                pred_df['Date'] = valid_target_date\n                pred_df['Target'] = return_df.loc[valid_target_date].values\n                pred_df['pred'] = weights.squeeze()\n                pred_df.fillna(0, inplace=True)\n                pred_df = add_rank_with_new_col_name(pred_df, col_name='pred', new_col_name='Rank')\n                pred_df = add_rank_with_new_col_name(pred_df, col_name='Target', new_col_name='TrueRank')\n                preds_df = pd.concat([preds_df, pred_df])\n                preds_df = preds_df.reset_index(drop=True)\n                print(pred_df)\n            sns.scatterplot(data=preds_df, x='pred', y='Target', hue='Date', alpha=0.7)\n            plt.figure(figsize=(8, 12))\n            plt.show()\n            score = calc_spread_return_sharpe(preds_df)\n            print('score:', score)\n            \n        valid_losses.append(batch_valid_loss/(k+1))\n        print(f'{i+1}\\t loss: ', losses[-1], '\\t', 'valid_loss: ', valid_losses[-1])\n#         if i %10 == 9:\n#             print(f'{i+1}\\t loss: ', losses[-1], '\\t', 'valid_loss: ', valid_losses[-1])\n        \n            \n            \n    plt.figure()\n    plt.plot(range(1, len(losses)+1), losses, label='train_loss')\n    plt.plot(range(1, len(valid_losses)+1), valid_losses, label='valid_loss')\n    plt.xlabel('iteration')\n    plt.legend()\n    plt.show()\n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:01:59.040172Z","iopub.execute_input":"2022-06-26T07:01:59.040418Z","iopub.status.idle":"2022-06-26T07:24:22.696458Z","shell.execute_reply.started":"2022-06-26T07:01:59.040390Z","shell.execute_reply":"2022-06-26T07:24:22.693672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save model\nmodel_path = 'residual_model.pth'\n\ntorch.save(model.state_dict(), model_path)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:24:22.697592Z","iopub.status.idle":"2022-06-26T07:24:22.697929Z","shell.execute_reply.started":"2022-06-26T07:24:22.697737Z","shell.execute_reply":"2022-06-26T07:24:22.697755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# evaluator","metadata":{}},{"cell_type":"markdown","source":"`calc_spread_return_sharpe`関数は[Train Demo](https://www.kaggle.com/code/smeitoma/train-demo)で紹介されている関数をそのまま使わせて頂いています。  \n推論した**Rank**と、正解の**Target**を含むデータフレームを渡すと、コンテストの評価方法に沿ったスコアを計算してくれます。","metadata":{}},{"cell_type":"code","source":"def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:24:22.700082Z","iopub.status.idle":"2022-06-26T07:24:22.700671Z","shell.execute_reply.started":"2022-06-26T07:24:22.700392Z","shell.execute_reply":"2022-06-26T07:24:22.700429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 予測用のデータフレームと、予測結果をもとに、スコアを計算する関数\ndef evaluator(df, pred):\n    df[\"pred\"] = pred\n    df = add_rank(df)\n    score = calc_spread_return_sharpe(df)\n    return score","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:24:22.702739Z","iopub.status.idle":"2022-06-26T07:24:22.703253Z","shell.execute_reply.started":"2022-06-26T07:24:22.703028Z","shell.execute_reply":"2022-06-26T07:24:22.703051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 提出","metadata":{}},{"cell_type":"markdown","source":"時系列APIを使って推論結果を登録します。  \n特徴量として、移動平均等の過去データを参照する値を採用しているため、時系列APIから得られたデータをため込む仕組みを実装する必要があります。  \nこの仕組みに関しては、[Start-to-finish demo based on s-meitoma + tweaks](https://www.kaggle.com/code/lowellniles/start-to-finish-demo-based-on-s-meitoma-tweaks)を参考にさせていただきました。  \n以下のコードでは`past_df`というデータフレームに履歴データをため込む実装になっています。","metadata":{}},{"cell_type":"code","source":"# 時系列APIのロード\nimport jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:24:22.705097Z","iopub.status.idle":"2022-06-26T07:24:22.705500Z","shell.execute_reply.started":"2022-06-26T07:24:22.705290Z","shell.execute_reply":"2022-06-26T07:24:22.705315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# supplemental filesを履歴データの初期状態としてセットアップ\npast_df = base_df[base_df['Date'] >= '2020-12-23'].copy()\npast_df = past_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T07:24:22.706502Z","iopub.status.idle":"2022-06-26T07:24:22.706828Z","shell.execute_reply.started":"2022-06-26T07:24:22.706653Z","shell.execute_reply":"2022-06-26T07:24:22.706674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}